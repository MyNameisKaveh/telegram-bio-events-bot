import asyncio
import re
import logging
from datetime import datetime
import html # Ø¨Ø±Ø§ÛŒ escape/unescape Ú©Ø±Ø¯Ù† HTML entities
import aiohttp
import feedparser
from telegram import Bot
from telegram.constants import ParseMode # ParseMode Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
import os
from dataclasses import dataclass
from typing import List, Optional, Tuple
from bs4 import BeautifulSoup, NavigableString, Tag # BeautifulSoup Ùˆ Ø§Ø¬Ø²Ø§ÛŒØ´ Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
from aiohttp import web

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class EventInfo:
    title: str
    description: str  # HTML Ø®Ø§Ù… Ø§Ø² ÙÛŒØ¯
    link: str
    published: str
    source_channel: str
    source_channel_username: Optional[str] = None

class EventDetector:
    # (Ú©Ø¯ EventDetector Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø§Ø² Ù¾Ø§Ø³Ø® Ù‚Ø¨Ù„ÛŒ Ú©Ù‡ Ø´Ø§Ù…Ù„ BeautifulSoup Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø¨ÙˆØ¯ØŒ Ø¨Ø§Ù‚ÛŒ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯)
    # ... (Ú©Ø¯ EventDetector Ø§Ø² Ù¾Ø§Ø³Ø® Ù‚Ø¨Ù„ÛŒ Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ Ú©Ù¾ÛŒ Ú©Ù†ÛŒØ¯) ...
    EVENT_KEYWORDS = [
        'ÙˆØ¨ÛŒÙ†Ø§Ø±', 'webinar', 'Ú©Ø§Ø±Ú¯Ø§Ù‡', 'workshop', 'Ø³Ù…ÛŒÙ†Ø§Ø±', 'seminar', 'Ú©Ù†ÙØ±Ø§Ù†Ø³', 'conference', 
        'Ù‡Ù…Ø§ÛŒØ´', 'congress', 'Ù†Ø´Ø³Øª', 'meeting', 'Ø¯ÙˆØ±Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ', 'course', 'Ú©Ù„Ø§Ø³', 'class', 
        'Ø§ÛŒÙˆÙ†Øª', 'event', 'Ø¨Ø±Ú¯Ø²Ø§Ø±', 'organize', 'Ø´Ø±Ú©Øª', 'participate', 'Ø«Ø¨Øª Ù†Ø§Ù…', 'register',
        'Ø±Ø§ÛŒÚ¯Ø§Ù†', 'free', 'Ø¢Ù†Ù„Ø§ÛŒÙ†', 'online', 'Ù…Ø¬Ø§Ø²ÛŒ', 'virtual', 'Ø¢Ù…ÙˆØ²Ø´', 'training', 
        'ÙØ±Ø§Ø®ÙˆØ§Ù†', 'call', 'Ú¯ÙˆØ§Ù‡ÛŒ', 'certificate', 'Ù…Ø¯Ø±Ú©', 'certification', 'Ù„Ø§ÛŒÙˆ', 'live'
    ]

    def detect_event(self, title: str, description_html: str) -> bool:
        soup = BeautifulSoup(description_html, "html.parser")
        first_p_tag = soup.find('p')
        if first_p_tag and first_p_tag.get_text(strip=True).lower().startswith("forwarded from"):
            first_p_tag.decompose()
        
        text_only_description = soup.get_text(separator=' ', strip=True)
        
        title_lower = title.lower()
        strong_title_keywords = ['ÙˆØ¨ÛŒÙ†Ø§Ø±', 'Ú©Ø§Ø±Ú¯Ø§Ù‡', 'Ø³Ù…ÛŒÙ†Ø§Ø±', 'Ø¯ÙˆØ±Ù‡', 'Ú©Ù†ÙØ±Ø§Ù†Ø³', 'Ù‡Ù…Ø§ÛŒØ´', 'Ù†Ø´Ø³Øª Ø¢Ù…ÙˆØ²Ø´ÛŒ', 'Ú©Ù„Ø§Ø³ Ø¢Ù†Ù„Ø§ÛŒÙ†']
        if any(keyword in title_lower for keyword in strong_title_keywords):
            return True

        full_text_desc_lower = text_only_description.lower()
        combined_text_lower = f"{title_lower} {full_text_desc_lower}"
        matches = sum(1 for keyword in self.EVENT_KEYWORDS if keyword in combined_text_lower)
        
        has_specific_pattern = any([
            'Ø«Ø¨Øª Ù†Ø§Ù…' in combined_text_lower, 'Ø´Ø±Ú©Øª Ø¯Ø±' in combined_text_lower,
            'Ù„ÛŒÙ†Ú© Ø«Ø¨Øª Ù†Ø§Ù…' in combined_text_lower, 'Ø¬Ù‡Øª Ø«Ø¨Øª Ù†Ø§Ù…' in combined_text_lower,
            'Ù‡Ø²ÛŒÙ†Ù‡ Ø¯ÙˆØ±Ù‡' in full_text_desc_lower, 'Ø³Ø±ÙØµÙ„ Ù‡Ø§ÛŒ Ø¯ÙˆØ±Ù‡' in full_text_desc_lower,
            'Ù…Ø¯Ø±Ø³ Ø¯ÙˆØ±Ù‡' in full_text_desc_lower, 'Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒØ´ØªØ± Ùˆ Ø«Ø¨Øª Ù†Ø§Ù…' in full_text_desc_lower,
            'register for' in combined_text_lower, 'join this' in combined_text_lower
        ])
        
        title_has_keyword = any(keyword in title_lower for keyword in ['Ø¢Ù…ÙˆØ²Ø´', 'ÙØ±Ø§Ø®ÙˆØ§Ù†', 'Ù„Ø§ÛŒÙˆ'])
        return title_has_keyword or has_specific_pattern or matches >= 2


class RSSTelegramBot:
    def __init__(self, bot_token: str, target_channel: str):
        self.bot_token = bot_token
        self.target_channel = target_channel
        self.bot = Bot(token=bot_token)
        self.detector = EventDetector()
        self.processed_items = set()
        self.rss_feeds = [
            # ... Ù„ÛŒØ³Øª ÙÛŒØ¯Ù‡Ø§ÛŒ Ø´Ù…Ø§ ...
            {'name': 'WinCell Co', 'url': 'https://rsshub.app/telegram/channel/wincellco', 'channel': 'wincellco'},
            {'name': 'Rayazistazma', 'url': 'https://rsshub.app/telegram/channel/Rayazistazma', 'channel': 'Rayazistazma'},
            {'name': 'SBU Bio Society', 'url': 'https://rsshub.app/telegram/channel/SBUBIOSOCIETY', 'channel': 'SBUBIOSOCIETY'},
            {'name': 'Test BioPy Channel', 'url': 'https://rsshub.app/telegram/channel/testbiopy', 'channel': 'testbiopy'}
        ]
        # ØªÚ¯â€ŒÙ‡Ø§ÛŒ HTML Ù…Ø¬Ø§Ø² Ø¯Ø± ØªÙ„Ú¯Ø±Ø§Ù… Ùˆ Ø§ØªØ±ÛŒØ¨ÛŒÙˆØªâ€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ø§Ø² Ø¨Ø±Ø§ÛŒ ØªÚ¯ <a>
        self.ALLOWED_TAGS = {
            'b': [], 'strong': [], 'i': [], 'em': [], 'u': [], 's': [], 
            'strike': [], 'del': [], 'code': [], 'pre': [], 
            'a': ['href'], 'span': ['class'] # span ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ tg-spoiler
        }
        self.RLM = "\u200F"


    async def fetch_feed(self, session: aiohttp.ClientSession, feed_info: dict) -> List[EventInfo]:
        # (Ø§ÛŒÙ† Ù…ØªØ¯ Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø¢Ø®Ø±ÛŒÙ† Ù†Ø³Ø®Ù‡ Ú©Ø§Ù…Ù„ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ø¨Ø§Ù‚ÛŒ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯)
        # ... (Ú©Ø¯ fetch_feed Ø§Ø² Ù¾Ø§Ø³Ø® Ù‚Ø¨Ù„ÛŒ Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ Ú©Ù¾ÛŒ Ú©Ù†ÛŒØ¯) ...
        events = []
        feed_url, feed_name = feed_info['url'], feed_info['name']
        logger.info(f"Fetching feed: {feed_name} from {feed_url}")
        try:
            headers = {'Cache-Control': 'no-cache', 'Pragma': 'no-cache'}
            async with session.get(feed_url, timeout=45, headers=headers) as response:
                if response.status == 200:
                    content = await response.text()
                    feed = feedparser.parse(content)
                    logger.info(f"Fetched {feed_name}. Entries: {len(feed.entries)}. LastBuildDate: {feed.feed.get('updated', feed.feed.get('published', 'N/A'))}")
                    for entry in feed.entries[:7]: 
                        entry_id = f"{feed_info.get('channel', feed_name)}_{entry.get('id', entry.get('link', ''))}"
                        if entry_id not in self.processed_items:
                            raw_title = entry.get('title', '').strip()
                            raw_description_html = entry.get('description', entry.get('summary', ''))
                            if self.detector.detect_event(raw_title, raw_description_html):
                                event = EventInfo(
                                    title=raw_title, description=raw_description_html,
                                    link=entry.get('link', ''), published=entry.get('published', ''),
                                    source_channel=feed_name, source_channel_username=feed_info.get('channel')
                                )
                                events.append(event)
                                self.processed_items.add(entry_id)
                        if len(self.processed_items) > 1500:
                            self.processed_items = set(list(self.processed_items)[500:])
                            logger.info(f"Cleaned up processed_items. New size: {len(self.processed_items)}")
                else:
                    logger.error(f"Error fetching {feed_name}: Status {response.status} - {await response.text(encoding='utf-8', errors='ignore')}")
        except Exception as e:
            logger.error(f"Exception in fetch_feed for {feed_name} ({feed_url}): {e}", exc_info=True)
        return events

    def _sanitize_href(self, url: Optional[str]) -> str:
        if url:
            url = url.strip()
            # ÙÙ‚Ø· URL Ù‡Ø§ÛŒ Ø§Ù…Ù† Ø±Ø§ Ø§Ø¬Ø§Ø²Ù‡ Ø¨Ø¯Ù‡
            if url.lower().startswith(('http://', 'https://', 'mailto:', 'tg://')):
                return html.escape(url, quote=True) # escape Ú©Ø±Ø¯Ù† Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø¯Ø§Ø± Ø§ØªØ±ÛŒØ¨ÛŒÙˆØª
        return ""

    def _convert_node_to_telegram_html(self, node) -> str:
        """Ø¨Ù‡ ØµÙˆØ±Øª Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ ÛŒÚ© Ú¯Ø±Ù‡ BeautifulSoup Ø±Ø§ Ø¨Ù‡ Ø±Ø´ØªÙ‡ HTML Ø§Ù…Ù† Ø¨Ø±Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù… ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
        if isinstance(node, NavigableString):
            return html.escape(str(node)) # Ù…ØªÙ†â€ŒÙ‡Ø§ Ø¨Ø§ÛŒØ¯ escape Ø´ÙˆÙ†Ø¯

        if node.name == 'br':
            return '\n' # <br> Ø¨Ù‡ Ø®Ø· Ø¬Ø¯ÛŒØ¯ ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒØ´ÙˆØ¯

        # Ù…Ø¯ÛŒØ±ÛŒØª ØªÚ¯â€ŒÙ‡Ø§ÛŒ Ø¨Ù„Ø§Ú© Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ØµÙ„Ù‡ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù
        # ØªÙ„Ú¯Ø±Ø§Ù… ØªÚ¯ <p> Ø±Ø§ Ø¨Ù‡ Ø·ÙˆØ± Ø®Ø§Øµ Ø¨Ø±Ø§ÛŒ ÙØ§ØµÙ„Ù‡ Ù†Ù…ÛŒâ€ŒØ´Ù†Ø§Ø³Ø¯ØŒ Ø¨Ø§ÛŒØ¯ Ø§Ø² \n\n Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ…
        # Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ ÙÙ‚Ø· Ù…Ø­ØªÙˆØ§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯ØŒ ÙØ§ØµÙ„Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø¨ÛŒÙ† Ø¨Ù„Ø§Ú©â€ŒÙ‡Ø§ Ø¨Ø§ÛŒØ¯ Ø¯Ø± Ø³Ø·Ø­ Ø¨Ø§Ù„Ø§ØªØ± Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯
        # ÛŒØ§ Ø§ÛŒÙ†Ú©Ù‡ Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ø¹Ø¯ Ø§Ø² Ù‡Ø± Ø¨Ù„Ø§Ú© \n\n Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒÙ… Ùˆ Ø³Ù¾Ø³ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒÙ….

        # Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ±Ø²Ù†Ø¯Ø§Ù†
        children_html = "".join(self._convert_node_to_telegram_html(child) for child in node.children)

        tag_name = node.name
        if tag_name in self.ALLOWED_TAGS:
            attrs_str = ""
            if tag_name == 'a':
                href = self._sanitize_href(node.get('href'))
                if href:
                    attrs_str = f' href="{href}"'
                else: # Ø§Ú¯Ø± Ù„ÛŒÙ†Ú© Ù…Ø¹ØªØ¨Ø± Ù†Ø¨ÙˆØ¯ØŒ ØªÚ¯ a Ø±Ø§ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ø¨Ú¯ÛŒØ± Ùˆ ÙÙ‚Ø· Ù…Ø­ØªÙˆØ§ÛŒØ´ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†
                    return children_html 
            elif tag_name == 'span':
                # ÙÙ‚Ø· Ø§Ø³Ù¾ÙˆÛŒÙ„Ø± ØªÙ„Ú¯Ø±Ø§Ù… Ù…Ø¬Ø§Ø² Ø§Ø³Øª
                if node.get('class') == ['tg-spoiler']:
                    attrs_str = ' class="tg-spoiler"'
                else: # Ø³Ø§ÛŒØ± span Ù‡Ø§ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
                    return children_html
            elif tag_name == 'pre':
                 # Ø¨Ø±Ø§ÛŒ <pre>ØŒ Ø§Ú¯Ø± Ø¯Ø§Ø®Ù„Ø´ <code> Ø¨Ø§ Ú©Ù„Ø§Ø³ Ø²Ø¨Ø§Ù† Ø¨ÙˆØ¯ØŒ Ø¢Ù† Ø±Ø§ Ù‡Ù… Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±
                code_child = node.find('code', class_=lambda x: x and isinstance(x, list) and x[0].startswith('language-'))
                if code_child:
                    lang = html.escape(code_child['class'][0].split('-',1)[1], quote=True)
                    # Ù…Ø­ØªÙˆØ§ÛŒ Ú©Ø¯ Ø¨Ø§ÛŒØ¯ escape Ø´ÙˆØ¯. children_html Ø§Ø² Ù‚Ø¨Ù„ Ø§ÛŒÙ† Ú©Ø§Ø± Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ code_child Ø§Ù†Ø¬Ø§Ù… Ø¯Ø§Ø¯Ù‡
                    return f'<pre><code class="language-{lang}">{children_html}</code></pre>'
                # Ø§Ú¯Ø± Ú©Ø¯ Ø³Ø§Ø¯Ù‡ Ø¨ÙˆØ¯ ÛŒØ§ Ø¨Ø¯ÙˆÙ† Ú©Ù„Ø§Ø³ Ø²Ø¨Ø§Ù†
                return f"<pre>{children_html}</pre>"
            elif tag_name == 'code' and node.parent.name == 'pre':
                 # Ø§Ú¯Ø± code Ø¯Ø§Ø®Ù„ pre Ø¨ÙˆØ¯ØŒ Ø®ÙˆØ¯ pre ØªÚ¯ Ø±Ø§ Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯ØŒ Ø§ÛŒÙ†Ø¬Ø§ ÙÙ‚Ø· Ù…Ø­ØªÙˆØ§ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†
                 return children_html


            # Ø¨Ø±Ø§ÛŒ Ø³Ø§ÛŒØ± ØªÚ¯â€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ø§Ø² Ù…Ø§Ù†Ù†Ø¯ b, i, u, s
            return f"<{tag_name}{attrs_str}>{children_html}</{tag_name}>"
        
        # Ø§Ú¯Ø± ØªÚ¯ Ù…Ø¬Ø§Ø² Ù†Ø¨ÙˆØ¯ØŒ ÙÙ‚Ø· Ù…Ø­ØªÙˆØ§ÛŒ ÙØ±Ø²Ù†Ø¯Ø§Ù†Ø´ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù† (unwrap)
        return children_html

    def _prepare_description_telegram_html(self, html_content: str) -> str:
        if not html_content:
            return ""
        soup = BeautifulSoup(html_content, "html.parser")

        # 1. Ø­Ø°Ù Ø¨Ø®Ø´ "Forwarded From"
        first_p = soup.find('p')
        if first_p and first_p.get_text(strip=True).lower().startswith("forwarded from"):
            first_p.decompose()
        
        # 2. ØªØ¨Ø¯ÛŒÙ„ Ú©Ù„ Ù…Ø­ØªÙˆØ§ÛŒ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø¨Ù‡ HTML Ø³Ø§Ø²Ú¯Ø§Ø± Ø¨Ø§ ØªÙ„Ú¯Ø±Ø§Ù…
        # Ø§ÛŒÙ† Ú©Ø§Ø± Ø±Ø§ Ø¨Ø§ Ù¾ÛŒÙ…Ø§ÛŒØ´ ÙØ±Ø²Ù†Ø¯Ø§Ù† body ÛŒØ§ Ø®ÙˆØ¯ soup Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…
        # Ùˆ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¨Ù„Ø§Ú© Ø§ØµÙ„ÛŒ (Ù…Ø«Ù„ <p> Ù‡Ø§ÛŒ Ø³Ø§Ø¨Ù‚) ÛŒÚ© \n\n Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
        
        parts = []
        # Ø¨Ù‡ Ø¬Ø§ÛŒ Ù¾ÛŒÙ…Ø§ÛŒØ´ ØªÚ¯â€ŒÙ‡Ø§ÛŒ Ø®Ø§ØµØŒ Ú©Ù„ Ø¨Ø¯Ù†Ù‡ Ø±Ø§ Ø¨Ù‡ ØªØ§Ø¨Ø¹ Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…
        # Ùˆ Ø³Ù¾Ø³ \n Ù‡Ø§ Ø±Ø§ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
        # Ø§ÛŒÙ† Ø±ÙˆØ´ Ø¨Ù‡ Ø­ÙØ¸ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø±Ø®ØªÛŒ Ùˆ ØªÙˆØ¯Ø±ØªÙˆ Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        
        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ø³Ø§Ø®ØªØ§Ø±Ù‡Ø§ÛŒ Ø¨Ù„Ø§Ú© Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ \n
        # Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¯Ù‚Øª Ø²ÛŒØ§Ø¯ÛŒ Ø¯Ø§Ø±Ø¯ ØªØ§ Ø³Ø§Ø®ØªØ§Ø± Ø­ÙØ¸ Ø´ÙˆØ¯
        # Ø¨Ø±Ø§ÛŒ Ù…Ø«Ø§Ù„ØŒ Ø¨Ø¹Ø¯ Ø§Ø² Ù‡Ø± ØªÚ¯ p ÛŒØ§ div Ú©Ù‡ Ø¯Ø± Ø±ÛŒØ´Ù‡ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯ØŒ Ø¯Ùˆ Ø®Ø· Ø¬Ø¯ÛŒØ¯ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
        body_content_tags = soup.find_all(['p', 'div'], recursive=False) # ÙÙ‚Ø· ØªÚ¯â€ŒÙ‡Ø§ÛŒ Ø³Ø·Ø­ Ø§ÙˆÙ„
        if not body_content_tags and soup.body: # Ø§Ú¯Ø± ØªÚ¯ body Ø¨ÙˆØ¯ ÙˆÙ„ÛŒ p, div Ù…Ø³ØªÙ‚ÛŒÙ… Ù†Ø¯Ø§Ø´Øª
            body_content_tags = soup.body.contents
        elif not body_content_tags: # Ø§Ú¯Ø± body Ù‡Ù… Ù†Ø¨ÙˆØ¯ØŒ Ø§Ø² Ø®ÙˆØ¯ soup Ø´Ø±ÙˆØ¹ Ú©Ù†
             body_content_tags = soup.contents

        for element in body_content_tags:
            converted_html_part = self._convert_node_to_telegram_html(element)
            if converted_html_part.strip(): # ÙÙ‚Ø· Ø§Ú¯Ø± Ø¨Ø®Ø´ÛŒ Ù…Ø­ØªÙˆØ§ Ø¯Ø§Ø´Øª
                parts.append(converted_html_part)

        # Ø§ØªØµØ§Ù„ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ø´Ø¯Ù‡ Ø¨Ø§ Ø¯Ùˆ Ø®Ø· Ø¬Ø¯ÛŒØ¯ (Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù)
        # Ø§ÛŒÙ† Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨ÛŒØ´ Ø§Ø² Ø­Ø¯ \n\n Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†Ø¯ Ø§Ú¯Ø± Ø®ÙˆØ¯ _convert_node_to_telegram_html Ù‡Ù… \n Ú¯Ø°Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.
        # Ø¨Ù‡ØªØ± Ø§Ø³Øª _convert_node_to_telegram_html ÙÙ‚Ø· ØªÚ¯â€ŒÙ‡Ø§ Ø±Ø§ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†Ø¯ Ùˆ \n Ù‡Ø§ Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ù†ÛŒÙ….
        # ÙØ¹Ù„Ø§ Ø¨Ø§ \n Ø³Ø§Ø¯Ù‡ join Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ùˆ Ø³Ù¾Ø³ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
        
        final_html = "\n".join(parts).strip()
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø®Ø·ÙˆØ· Ø¬Ø¯ÛŒØ¯: Ø¨ÛŒØ´ Ø§Ø² Ø¯Ùˆ \n Ù…ØªÙˆØ§Ù„ÛŒ Ø±Ø§ Ø¨Ù‡ Ø¯Ùˆ \n ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†
        final_html = re.sub(r'\n\s*\n\s*\n+', '\n\n', final_html)
        # Ø­Ø°Ù Ø®Ø·ÙˆØ· Ø®Ø§Ù„ÛŒ Ú©Ù‡ Ù…Ù…Ú©Ù† Ø§Ø³Øª ÙÙ‚Ø· Ø´Ø§Ù…Ù„ RLM Ø¨Ø§Ø´Ù†Ø¯
        final_html = "\n".join(line for line in final_html.splitlines() if line.strip() != self.RLM and line.strip())

        return final_html.strip()


    def format_event_message(self, event: EventInfo) -> str:
        display_title = event.title.strip()
        
        # ØªØ¨Ø¯ÛŒÙ„ ØªÙˆØ¶ÛŒØ­Ø§Øª Ø¨Ù‡ HTML Ø³Ø§Ø²Ú¯Ø§Ø± Ø¨Ø§ ØªÙ„Ú¯Ø±Ø§Ù…
        description_telegram_html = self._prepare_description_telegram_html(event.description)
        
        # Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ ØªÚ©Ø±Ø§Ø±ØŒ ÛŒÚ© Ù†Ø³Ø®Ù‡ Ù…ØªÙ†ÛŒ Ø§Ø² ØªÙˆØ¶ÛŒØ­Ø§Øª Ùˆ Ø¹Ù†ÙˆØ§Ù† Ù†ÛŒØ§Ø² Ø¯Ø§Ø±ÛŒÙ…
        temp_soup_desc = BeautifulSoup(description_telegram_html, "html.parser") # Ø§Ø² HTML ØªØ¨Ø¯ÛŒÙ„ Ø´Ø¯Ù‡ØŒ Ù…ØªÙ† Ø¨Ú¯ÛŒØ±
        description_plain_text_for_check = temp_soup_desc.get_text(separator=' ', strip=True)

        description_to_display_html = description_telegram_html
        
        if description_plain_text_for_check and display_title:
            leading_symbols_pattern = r"^[ğŸ”ğŸ–¼âšœï¸ğŸ“ğŸ“¢âœ”ï¸âœ…ğŸ”†ğŸ—“ï¸ğŸ“ğŸ’³#Ùªâ™¦ï¸ğŸ”¹ğŸ”¸ğŸŸ¢â™¦ï¸â–ªï¸â–«ï¸â–ªï¸â€¢â—ğŸ”˜ğŸ‘â€ğŸ—¨\s]*(?=[^\s])"
            trailing_punctuation_pattern = r"[\s.:â€¦]+$"
            def normalize_text(text):
                if not text: return ""
                text = re.sub(leading_symbols_pattern, "", text, flags=re.IGNORECASE).strip()
                text = re.sub(trailing_punctuation_pattern, "", text)
                return text.lower().strip()

            title_comp = normalize_text(display_title)
            if title_comp:
                first_desc_line_plain = description_plain_text_for_check.split('\n', 1)[0].strip()
                first_desc_line_comp = normalize_text(first_desc_line_plain)

                if first_desc_line_comp and \
                   (title_comp == first_desc_line_comp or \
                   (len(title_comp) >= 8 and first_desc_line_comp.startswith(title_comp)) or \
                   (len(first_desc_line_comp) >= 8 and title_comp.startswith(first_desc_line_comp))):
                    
                    logger.info(f"HTML MODE - Title ('{display_title}') considered redundant with first line of desc ('{first_desc_line_plain}'). Adjusting.")
                    # Ø¨Ø±Ø§ÛŒ HTMLØŒ Ø­Ø°Ù Ø®Ø· Ø§ÙˆÙ„ Ú©Ù…ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒØªØ± Ø§Ø³Øª.
                    # Ø³Ø§Ø¯Ù‡â€ŒØªØ±ÛŒÙ† Ú©Ø§Ø± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ø§Ú¯Ø± ØªÚ©Ø±Ø§Ø± Ø¨ÙˆØ¯ØŒ Ú©Ù„ ØªÙˆØ¶ÛŒØ­Ø§Øª Ø±Ø§ Ù†Ù…Ø§ÛŒØ´ Ù†Ø¯Ù‡ÛŒÙ… Ø§Ú¯Ø± Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡ Ø¨ÙˆØ¯ØŒ
                    # ÛŒØ§ Ø§Ú¯Ø± Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø¨ÙˆØ¯ØŒ Ø³Ø¹ÛŒ Ú©Ù†ÛŒÙ… Ø¨Ø®Ø´ Ø§ÙˆÙ„ Ø±Ø§ Ø­Ø°Ù Ú©Ù†ÛŒÙ… (Ú©Ù‡ Ø³Ø§Ø¯Ù‡ Ù†ÛŒØ³Øª Ø¯Ø± HTML)
                    # ÙØ¹Ù„Ø§ Ø§Ú¯Ø± ØªÚ©Ø±Ø§Ø± Ø¨ÙˆØ¯ Ùˆ ØªÙˆØ¶ÛŒØ­Ø§Øª ÙÙ‚Ø· Ù‡Ù…Ø§Ù† Ø®Ø· Ø¨ÙˆØ¯ØŒ Ø®Ø§Ù„ÛŒâ€ŒØ§Ø´ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
                    # Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¯Ø§Ø±Ø¯: Ú†Ú¯ÙˆÙ†Ù‡ ÛŒÚ© "Ø®Ø·" Ø±Ø§ Ø§Ø² Ø±Ø´ØªÙ‡ HTML Ø­Ø°Ù Ú©Ù†ÛŒÙ….
                    # Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø¯Ú¯ÛŒØŒ Ø§Ú¯Ø± Ø®Ø· Ø§ÙˆÙ„ ØªÙˆØ¶ÛŒØ­Ø§Øª (Ø¨Ù‡ ØµÙˆØ±Øª Ù…ØªÙ†ÛŒ) Ø¨Ø§ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ©ÛŒ Ø¨ÙˆØ¯ØŒ Ú©Ù„ ØªÙˆØ¶ÛŒØ­Ø§Øª HTML Ø±Ø§ Ù†Ú¯Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ø±ÛŒÙ…
                    # Ùˆ ØµØ±ÙØ§ Ø¹Ù†ÙˆØ§Ù† Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ø±Ø§ Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ… ÛŒØ§ Ø¨Ø§ÛŒØ¯ Ø±Ø§Ù‡ÛŒ Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù Ø¨Ø®Ø´ Ø§ÙˆÙ„ HTML Ù¾ÛŒØ¯Ø§ Ú©Ù†ÛŒÙ….
                    # ÙØ¹Ù„Ø§ØŒ Ø§Ú¯Ø± ØªÚ©Ø±Ø§Ø± Ø¨ÙˆØ¯ØŒ Ù‡Ù…Ø§Ù† Ù…Ù†Ø·Ù‚ Ù‚Ø¨Ù„ÛŒ (Ø­Ø°Ù Ø®Ø· Ø§ÙˆÙ„ Ø§Ø² Ù†Ø³Ø®Ù‡ Ù…ØªÙ†ÛŒ Ùˆ Ø³Ù¾Ø³ ÙØ±Ù…Øª Ù…Ø¬Ø¯Ø¯) Ø±Ø§ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¨Ù‡ Ø³Ø§Ø¯Ú¯ÛŒ Ø±ÙˆÛŒ HTML Ø§Ø¹Ù…Ø§Ù„ Ú©Ø±Ø¯.
                    # Ù¾Ø³ØŒ Ø§Ú¯Ø± ØªÚ©Ø±Ø§Ø± Ø¨ÙˆØ¯ØŒ Ø¹Ù†ÙˆØ§Ù† Ø±Ø§ Ù†Ù…Ø§ÛŒØ´ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ… Ùˆ ØªÙˆØ¶ÛŒØ­Ø§Øª Ø±Ø§ Ù‡Ù… Ú©Ø§Ù…Ù„ Ù…ÛŒâ€ŒØ¢ÙˆØ±ÛŒÙ…ØŒ ÛŒØ§ Ø³Ø¹ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ø§Ø² ØªÙˆØ¶ÛŒØ­Ø§Øª Ú©Ù… Ú©Ù†ÛŒÙ….
                    # Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ø§Ø²Ù†Ú¯Ø±ÛŒ Ø§Ø³Ø§Ø³ÛŒ Ø¯Ø§Ø±Ø¯ Ø§Ú¯Ø± Ø¨Ø®ÙˆØ§Ù‡ÛŒÙ… Ø¨Ø®Ø´ Ø§ÙˆÙ„ ÛŒÚ© Ø±Ø´ØªÙ‡ HTML Ø±Ø§ Ø­Ø°Ù Ú©Ù†ÛŒÙ….
                    # Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù†Ø³Ø®Ù‡ØŒ Ø§Ú¯Ø± ØªÚ©Ø±Ø§Ø± Ø¨ÙˆØ¯ØŒ ÙÙ‚Ø· Ù„Ø§Ú¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ… Ùˆ Ù‡Ø± Ø¯Ùˆ Ø±Ø§ Ù†Ù…Ø§ÛŒØ´ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…ØŒ Ú†ÙˆÙ† Ø­Ø°Ù Ø§Ø² HTML Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø§Ø³Øª.
                    # ÛŒØ§ Ø§ÛŒÙ†Ú©Ù‡ØŒ Ø§Ú¯Ø± Ø¹Ù†ÙˆØ§Ù† ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨ÙˆØ¯ØŒ Ø®ÙˆØ¯ Ø¹Ù†ÙˆØ§Ù† Ø±Ø§ Ø­Ø°Ù Ú©Ù†ÛŒÙ… Ùˆ Ø¨Ú¯Ø°Ø§Ø±ÛŒÙ… ØªÙˆØ¶ÛŒØ­Ø§Øª Ø´Ø§Ù…Ù„ Ø¢Ù† Ø¨Ø§Ø´Ø¯.
                    # **ØªØµÙ…ÛŒÙ… ÙØ¹Ù„ÛŒ: Ø§Ú¯Ø± ØªÚ©Ø±Ø§Ø± Ø¨ÙˆØ¯ØŒ Ø®Ø· Ø§ÙˆÙ„ Ø±Ø§ Ø§Ø² description_telegram_html (Ø§Ú¯Ø± Ø¨Ø§ <br> Ø¬Ø¯Ø§ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯) Ø­Ø°Ù Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….**
                    # Ø§ÛŒÙ† Ú©Ø§Ø± Ø¨Ø³ÛŒØ§Ø± ØªÙ‚Ø±ÛŒØ¨ÛŒ Ø§Ø³Øª.
                    if title_comp == first_desc_line_comp: # ÙÙ‚Ø· Ø§Ú¯Ø± Ø¯Ù‚ÛŒÙ‚Ø§ ÛŒÚ©ÛŒ Ø¨ÙˆØ¯Ù†Ø¯
                        if '\n' in description_telegram_html: # ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… \n Ø¬Ø¯Ø§Ú©Ù†Ù†Ø¯Ù‡ Ø®Ø· Ø§ÙˆÙ„ Ø§Ø³Øª
                            description_to_display_html = description_telegram_html.split('\n', 1)[1].strip()
                        else:
                            description_to_display_html = "" # Ú©Ù„ ØªÙˆØ¶ÛŒØ­Ø§Øª Ù‡Ù…Ø§Ù† Ø¹Ù†ÙˆØ§Ù† Ø¨ÙˆØ¯


        DESCRIPTION_MAX_LEN_HTML = 3800 # Ø¨Ø±Ø§ÛŒ HTML Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ú©Ø§Ø±Ø§Ú©ØªØ± Ú©Ù…ØªØ± Ø§Ø³Øª Ú†ÙˆÙ† Ø®ÙˆØ¯ ØªÚ¯â€ŒÙ‡Ø§ Ù‡Ù… ÙØ¶Ø§ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù†Ø¯
        if len(description_to_display_html) > DESCRIPTION_MAX_LEN_HTML:
            # Ú©ÙˆØªØ§Ù‡ Ú©Ø±Ø¯Ù† HTML Ø¨Ø¯ÙˆÙ† Ø´Ú©Ø³ØªÙ† ØªÚ¯â€ŒÙ‡Ø§ Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø§Ø³Øª. ÙØ¹Ù„Ø§ ÛŒÚ© Ú©ÙˆØªØ§Ù‡ Ú©Ø±Ø¯Ù† Ø³Ø§Ø¯Ù‡ Ù…ØªÙ†ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ….
            # Ø§ÛŒÙ† Ù…Ù…Ú©Ù† Ø§Ø³Øª HTML Ø±Ø§ Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ú©Ù†Ø¯. Ø±Ø§Ù‡ Ø¨Ù‡ØªØ±ØŒ Ú©ÙˆØªØ§Ù‡ Ú©Ø±Ø¯Ù† Ù‚Ø¨Ù„ Ø§Ø² ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ HTML Ø§Ø³Øª ÛŒØ§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡.
            temp_soup = BeautifulSoup(description_to_display_html, "html.parser")
            plain_text_for_truncate = temp_soup.get_text(separator=' ', strip=True)
            if len(plain_text_for_truncate) > DESCRIPTION_MAX_LEN_HTML: # Ø§Ú¯Ø± Ù…ØªÙ† Ø®Ø§Ù„Øµ Ù‡Ù… Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø¨ÙˆØ¯
                cut_off_point = plain_text_for_truncate.rfind('.', 0, DESCRIPTION_MAX_LEN_HTML - 20) # -20 Ø¨Ø±Ø§ÛŒ " (...)"
                if cut_off_point != -1:
                    truncated_plain_text = plain_text_for_truncate[:cut_off_point+1] + f"{self.RLM} (...)"
                else:
                    truncated_plain_text = plain_text_for_truncate[:DESCRIPTION_MAX_LEN_HTML - 20] + f"{self.RLM}..."
                # ØªØ¨Ø¯ÛŒÙ„ Ù…Ø¬Ø¯Ø¯ Ù…ØªÙ† Ú©ÙˆØªØ§Ù‡ Ø´Ø¯Ù‡ Ø¨Ù‡ HTML Ø³Ø§Ø¯Ù‡ (ÙÙ‚Ø· escape Ú©Ø±Ø¯Ù†)
                description_to_display_html = html.escape(truncated_plain_text) # Ø§ÛŒÙ† ÙØ±Ù…Øª ØºÙ†ÛŒ Ø±Ø§ Ø§Ø² Ø¯Ø³Øª Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
                logger.warning("Description was too long and has been truncated to plain text.")


        if not description_to_display_html.strip():
            description_to_display_html = ""

        # Ù…ÙˆÙ†ØªØ§Ú˜ Ù¾ÛŒØ§Ù… Ø¨Ø§ HTML
        message_parts = []
        if display_title:
             message_parts.append(f"{self.RLM}<b>{html.escape(display_title)}</b>")

        if description_to_display_html:
            separator = "\n\n" if display_title else ""
            # RLM Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ¯ ØªÙˆØ¶ÛŒØ­Ø§Øª Ù„Ø§Ø²Ù… Ù†ÛŒØ³Øª Ø§Ú¯Ø± Ú©Ù„ Ø¨Ù„ÙˆÚ© HTML Ø¬Ù‡Øª Ø¯Ø±Ø³ØªÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯
            # Ø§Ù…Ø§ Ø§Ú¯Ø± Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø´Ø±ÙˆØ¹ Ø´ÙˆØ¯ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù…ÙÛŒØ¯ Ø¨Ø§Ø´Ø¯.
            # ÙØ¹Ù„Ø§ RLM Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ú©Ù„ Ø¨Ù„ÙˆÚ© ØªÙˆØ¶ÛŒØ­Ø§Øª Ø§Ø¶Ø§ÙÙ‡ Ù†Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú†ÙˆÙ† Ø®ÙˆØ¯ HTML Ø¨Ø§ÛŒØ¯ Ø¬Ù‡Øª Ø±Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ù†Ø¯.
            message_parts.append(f"{separator}{description_to_display_html}")


        meta_info_parts = []
        if event.link:
            meta_info_parts.append(f"{self.RLM}ğŸ”— <a href=\"{self._sanitize_href(event.link)}\">Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ø§Ù…Ù„ Ø±ÙˆÛŒØ¯Ø§Ø¯</a>")
        
        if event.source_channel_username:
            meta_info_parts.append(f"{self.RLM}ğŸ“¢ <b>Ù…Ù†Ø¨Ø¹:</b> <a href=\"https://t.me/{html.escape(event.source_channel_username)}\">{html.escape(event.source_channel)}</a>")
        else:
            meta_info_parts.append(f"{self.RLM}ğŸ“¢ <b>Ù…Ù†Ø¨Ø¹:</b> {html.escape(event.source_channel)}")

        if event.published:
            formatted_date = event.published
            try:
                date_obj = datetime.strptime(event.published, "%a, %d %b %Y %H:%M:%S %Z")
                formatted_date = date_obj.strftime("%d %b %Y - %H:%M (%Z)")
            except: pass
            if formatted_date:
                 meta_info_parts.append(f"{self.RLM}ğŸ“… <b>Ø§Ù†ØªØ´Ø§Ø±:</b> {html.escape(formatted_date)}")

        if meta_info_parts:
            separator_meta = "\n\n" if message_parts else "" 
            message_parts.append(separator_meta + "\n".join(meta_info_parts)) # \n Ø¨ÛŒÙ† Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù…ØªØ§ Ú©Ø§ÙÛŒ Ø§Ø³Øª

        final_message_html = "\n".join(message_parts).strip()
                                                          
        TELEGRAM_MSG_MAX_LEN = 4096
        if len(final_message_html) > TELEGRAM_MSG_MAX_LEN:
            logger.warning(f"HTML Message for '{display_title}' too long ({len(final_message_html)} chars). Telegram might truncate or reject.")
            # Ú©ÙˆØªØ§Ù‡ Ú©Ø±Ø¯Ù† HTML Ø¨Ø¯ÙˆÙ† Ø´Ú©Ø³ØªÙ† ØªÚ¯â€ŒÙ‡Ø§ Ø¨Ø³ÛŒØ§Ø± Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø§Ø³Øª.
            # Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù†ÛŒØ§Ø² Ø¨Ù‡ ÛŒÚ© Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ ÛŒØ§ Ù…Ù†Ø·Ù‚ Ù‚ÙˆÛŒ Ø¨Ø±Ø§ÛŒ Ú©ÙˆØªØ§Ù‡ Ú©Ø±Ø¯Ù† HTML Ø¯Ø§Ø±Ø¯.
            # ÙØ¹Ù„Ø§ ÙÙ‚Ø· Ù‡Ø´Ø¯Ø§Ø± Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ….
            
        return final_message_html

    async def publish_event(self, event: EventInfo):
        try:
            message_html = self.format_event_message(event) # Ù…ØªØ¯ ÙØ±Ù…Øªâ€ŒØ¨Ù†Ø¯ÛŒ Ø§Ú©Ù†ÙˆÙ† HTML Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯
            
            is_message_effectively_empty = not message_html or \
                                          (not event.title and not self._prepare_description_telegram_html(event.description).strip()) # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ
            
            if is_message_effectively_empty:
                logger.info(f"Skipping due to effectively empty HTML message for event from {event.source_channel} (Title: {event.title[:30]}...).")
                return

            await self.bot.send_message(
                chat_id=self.target_channel, text=message_html,
                parse_mode=ParseMode.HTML, # <--- ØªØºÛŒÛŒØ± Ø¨Ù‡ ParseMode.HTML
                disable_web_page_preview=True 
            )
            logger.info(f"Published event (HTML): {event.title[:60]}... from {event.source_channel}")
        except Exception as e:
            logger.error(f"Failed to publish event ({event.title[:60]}...) using HTML mode: {e}", exc_info=True)


    async def run_monitoring_loop(self):
        logger.info("Starting RSS monitoring...")
        await asyncio.sleep(10) 
        while True:
            logger.info("Checking all feeds...")
            all_new_events = []
            async with aiohttp.ClientSession() as session:
                tasks = [self.fetch_feed(session, feed_info) for feed_info in self.rss_feeds]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                for result in results:
                    if isinstance(result, list): all_new_events.extend(result)
                    elif isinstance(result, Exception): logger.error(f"Feed task error: {result}", exc_info=result)
            
            if all_new_events:
                logger.info(f"Found {len(all_new_events)} new event(s).")
                for event_to_publish in all_new_events:
                    await self.publish_event(event_to_publish)
                    await asyncio.sleep(5) 
            else:
                logger.info("No new events found.")
            
            check_interval_seconds = 180 
            logger.info(f"Next check in {check_interval_seconds // 60} minutes.")
            await asyncio.sleep(check_interval_seconds)

# ... (Ú©Ù„Ø§Ø³ Config Ùˆ ØªÙˆØ§Ø¨Ø¹ health_check, start_web_server, main Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø§Ø² Ù¾Ø§Ø³Ø® Ù‚Ø¨Ù„ÛŒ) ...
class Config:
    BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')
    TARGET_CHANNEL = os.getenv('TARGET_CHANNEL')

rss_bot_instance: Optional[RSSTelegramBot] = None 

async def health_check(request):
    global rss_bot_instance
    if rss_bot_instance:
        return web.Response(text=f"Bot is running! Processed items: {len(rss_bot_instance.processed_items)}", status=200)
    return web.Response(text="Bot not fully initialized.", status=200)

async def start_web_server():
    app = web.Application()
    app.router.add_get('/health', health_check)
    app.router.add_get('/', health_check)
    runner = web.AppRunner(app)
    await runner.setup()
    port = int(os.environ.get("PORT", "7860"))
    site = web.TCPSite(runner, '0.0.0.0', port)
    try:
        await site.start()
        logger.info(f"Web server started on port {port}")
    except OSError as e:
        logger.error(f"Failed to start web server on port {port}: {e}.")

async def main():
    global rss_bot_instance
    if not Config.BOT_TOKEN or not Config.TARGET_CHANNEL:
        logger.critical("Missing TELEGRAM_BOT_TOKEN or TARGET_CHANNEL environment variables!")
        return
    
    logger.info("Application starting...")
    rss_bot_instance = RSSTelegramBot(bot_token=Config.BOT_TOKEN, target_channel=Config.TARGET_CHANNEL)
    try:
        bot_info = await rss_bot_instance.bot.get_me()
        logger.info(f"Bot started: @{bot_info.username}")
        web_server_task = asyncio.create_task(start_web_server())
        await rss_bot_instance.run_monitoring_loop()
    except KeyboardInterrupt:
        logger.info("Bot stopped by user.")
    except Exception as e:
        logger.error(f"Critical bot error in main: {e}", exc_info=True)
    finally:
        logger.info("Bot shutting down...")
        if 'web_server_task' in locals() and web_server_task and not web_server_task.done(): 
            web_server_task.cancel()
            try: await web_server_task
            except asyncio.CancelledError: logger.info("Web server task cancelled.")
        logger.info("Application fully stopped.")

if __name__ == "__main__":
    asyncio.run(main())
