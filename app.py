import asyncio
import re
import logging
from datetime import datetime
import html # Ø¨Ø±Ø§ÛŒ escape/unescape Ú©Ø±Ø¯Ù† HTML entities
import aiohttp
import feedparser
from telegram import Bot
from telegram.constants import ParseMode
import os
from dataclasses import dataclass
from typing import List, Optional
from bs4 import BeautifulSoup, NavigableString, Tag
from aiohttp import web

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class EventInfo:
    title: str
    description: str  # HTML Ø®Ø§Ù… Ø§Ø² ÙÛŒØ¯
    link: str
    published: str
    source_channel: str
    source_channel_username: Optional[str] = None

class EventDetector:
    # (Ú©Ø¯ EventDetector Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø§Ø² Ù¾Ø§Ø³Ø® Ù‚Ø¨Ù„ÛŒ Ø¨Ø§Ù‚ÛŒ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯)
    EVENT_KEYWORDS = [
        'ÙˆØ¨ÛŒÙ†Ø§Ø±', 'webinar', 'Ú©Ø§Ø±Ú¯Ø§Ù‡', 'workshop', 'Ø³Ù…ÛŒÙ†Ø§Ø±', 'seminar', 'Ú©Ù†ÙØ±Ø§Ù†Ø³', 'conference', 
        'Ù‡Ù…Ø§ÛŒØ´', 'congress', 'Ù†Ø´Ø³Øª', 'meeting', 'Ø¯ÙˆØ±Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ', 'course', 'Ú©Ù„Ø§Ø³', 'class', 
        'Ø§ÛŒÙˆÙ†Øª', 'event', 'Ø¨Ø±Ú¯Ø²Ø§Ø±', 'organize', 'Ø´Ø±Ú©Øª', 'participate', 'Ø«Ø¨Øª Ù†Ø§Ù…', 'register',
        'Ø±Ø§ÛŒÚ¯Ø§Ù†', 'free', 'Ø¢Ù†Ù„Ø§ÛŒÙ†', 'online', 'Ù…Ø¬Ø§Ø²ÛŒ', 'virtual', 'Ø¢Ù…ÙˆØ²Ø´', 'training', 
        'ÙØ±Ø§Ø®ÙˆØ§Ù†', 'call', 'Ú¯ÙˆØ§Ù‡ÛŒ', 'certificate', 'Ù…Ø¯Ø±Ú©', 'certification', 'Ù„Ø§ÛŒÙˆ', 'live'
    ]

    def detect_event(self, title: str, description_html: str) -> bool:
        soup = BeautifulSoup(description_html, "html.parser")
        first_p_tag = soup.find('p')
        if first_p_tag and first_p_tag.get_text(strip=True).lower().startswith("forwarded from"):
            first_p_tag.decompose()
        
        text_only_description = soup.get_text(separator=' ', strip=True)
        
        title_lower = title.lower()
        strong_title_keywords = ['ÙˆØ¨ÛŒÙ†Ø§Ø±', 'Ú©Ø§Ø±Ú¯Ø§Ù‡', 'Ø³Ù…ÛŒÙ†Ø§Ø±', 'Ø¯ÙˆØ±Ù‡', 'Ú©Ù†ÙØ±Ø§Ù†Ø³', 'Ù‡Ù…Ø§ÛŒØ´', 'Ù†Ø´Ø³Øª Ø¢Ù…ÙˆØ²Ø´ÛŒ', 'Ú©Ù„Ø§Ø³ Ø¢Ù†Ù„Ø§ÛŒÙ†']
        if any(keyword in title_lower for keyword in strong_title_keywords):
            return True

        full_text_desc_lower = text_only_description.lower()
        combined_text_lower = f"{title_lower} {full_text_desc_lower}"
        matches = sum(1 for keyword in self.EVENT_KEYWORDS if keyword in combined_text_lower)
        
        has_specific_pattern = any([
            'Ø«Ø¨Øª Ù†Ø§Ù…' in combined_text_lower, 'Ø´Ø±Ú©Øª Ø¯Ø±' in combined_text_lower,
            'Ù„ÛŒÙ†Ú© Ø«Ø¨Øª Ù†Ø§Ù…' in combined_text_lower, 'Ø¬Ù‡Øª Ø«Ø¨Øª Ù†Ø§Ù…' in combined_text_lower,
            'Ù‡Ø²ÛŒÙ†Ù‡ Ø¯ÙˆØ±Ù‡' in full_text_desc_lower, 'Ø³Ø±ÙØµÙ„ Ù‡Ø§ÛŒ Ø¯ÙˆØ±Ù‡' in full_text_desc_lower,
            'Ù…Ø¯Ø±Ø³ Ø¯ÙˆØ±Ù‡' in full_text_desc_lower, 'Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒØ´ØªØ± Ùˆ Ø«Ø¨Øª Ù†Ø§Ù…' in full_text_desc_lower,
            'register for' in combined_text_lower, 'join this' in combined_text_lower
        ])
        
        title_has_keyword = any(keyword in title_lower for keyword in ['Ø¢Ù…ÙˆØ²Ø´', 'ÙØ±Ø§Ø®ÙˆØ§Ù†', 'Ù„Ø§ÛŒÙˆ'])
        return title_has_keyword or has_specific_pattern or matches >= 2

class RSSTelegramBot:
    def __init__(self, bot_token: str, target_channel: str):
        self.bot_token = bot_token
        self.target_channel = target_channel
        self.bot = Bot(token=bot_token)
        self.detector = EventDetector()
        self.processed_items = set()
        self.rss_feeds = [
            {'name': 'WinCell Co', 'url': 'https://rsshub.app/telegram/channel/wincellco', 'channel': 'wincellco'},
            {'name': 'Rayazistazma', 'url': 'https://rsshub.app/telegram/channel/Rayazistazma', 'channel': 'Rayazistazma'},
            {'name': 'SBU Bio Society', 'url': 'https://rsshub.app/telegram/channel/SBUBIOSOCIETY', 'channel': 'SBUBIOSOCIETY'},
            {'name': 'Test BioPy Channel', 'url': 'https://rsshub.app/telegram/channel/testbiopy', 'channel': 'testbiopy'}
        ]
        self.ALLOWED_TAGS_TELEGRAM = { # ØªÚ¯â€ŒÙ‡Ø§ÛŒ HTML Ù…Ø¬Ø§Ø² ØªÙ„Ú¯Ø±Ø§Ù…
            'b': [], 'strong': [], 'i': [], 'em': [], 'u': [], 's': [], 
            'strike': [], 'del': [], 'code': [], 'pre': [], 
            'a': ['href'], 'span': ['class'] # span ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ tg-spoiler
        }
        self.RLM = "\u200F" # Right-to-Left Mark

    async def fetch_feed(self, session: aiohttp.ClientSession, feed_info: dict) -> List[EventInfo]:
        # (Ø§ÛŒÙ† Ù…ØªØ¯ Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø§Ø² Ù¾Ø§Ø³Ø® Ù‚Ø¨Ù„ÛŒ Ø¨Ø§Ù‚ÛŒ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯)
        events = []
        feed_url, feed_name = feed_info['url'], feed_info['name']
        logger.info(f"Fetching feed: {feed_name} from {feed_url}")
        try:
            headers = {'Cache-Control': 'no-cache', 'Pragma': 'no-cache'}
            async with session.get(feed_url, timeout=45, headers=headers) as response:
                if response.status == 200:
                    content = await response.text()
                    feed = feedparser.parse(content)
                    logger.info(f"Fetched {feed_name}. Entries: {len(feed.entries)}. LastBuildDate: {feed.feed.get('updated', feed.feed.get('published', 'N/A'))}")
                    for entry in feed.entries[:7]: 
                        entry_id = f"{feed_info.get('channel', feed_name)}_{entry.get('id', entry.get('link', ''))}"
                        if entry_id not in self.processed_items:
                            raw_title = entry.get('title', '').strip()
                            raw_description_html = entry.get('description', entry.get('summary', ''))
                            if self.detector.detect_event(raw_title, raw_description_html):
                                event = EventInfo(
                                    title=raw_title, description=raw_description_html,
                                    link=entry.get('link', ''), published=entry.get('published', ''),
                                    source_channel=feed_name, source_channel_username=feed_info.get('channel')
                                )
                                events.append(event)
                                self.processed_items.add(entry_id)
                        if len(self.processed_items) > 1500:
                            self.processed_items = set(list(self.processed_items)[500:])
                            logger.info(f"Cleaned up processed_items. New size: {len(self.processed_items)}")
                else:
                    logger.error(f"Error fetching {feed_name}: Status {response.status} - {await response.text(encoding='utf-8', errors='ignore')}")
        except Exception as e:
            logger.error(f"Exception in fetch_feed for {feed_name} ({feed_url}): {e}", exc_info=True)
        return events

    def _sanitize_href(self, url: Optional[str]) -> Optional[str]:
        if url:
            url = url.strip()
            if url.lower().startswith(('http://', 'https://', 'mailto:', 'tg://')):
                return html.escape(url, quote=True)
        return None

    def _recursive_html_to_telegram_html(self, element) -> str:
        """ Ú¯Ø±Ù‡ BeautifulSoup Ø±Ø§ Ø¨Ù‡ Ø±Ø´ØªÙ‡ HTML Ø§Ù…Ù† Ø¨Ø±Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù… (Ø¨Ø§ Ø­ÙØ¸ ØªÚ¯â€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ø§Ø²) ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. """
        if isinstance(element, NavigableString):
            return html.escape(str(element))
        
        if element.name == 'br':
            return '\n'

        # Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ±Ø²Ù†Ø¯Ø§Ù† Ø§Ø¨ØªØ¯Ø§
        children_html = "".join(self._recursive_html_to_telegram_html(child) for child in element.children)

        tag_name = element.name
        if tag_name in self.ALLOWED_TAGS_TELEGRAM:
            attrs = {}
            if tag_name == 'a':
                href = self._sanitize_href(element.get('href'))
                if not href: return children_html # Ø§Ú¯Ø± Ù„ÛŒÙ†Ú© Ù…Ø¹ØªØ¨Ø± Ù†Ø¨ÙˆØ¯ØŒ ÙÙ‚Ø· Ù…Ø­ØªÙˆØ§
                attrs['href'] = href
            elif tag_name == 'span':
                if element.get('class') == ['tg-spoiler']:
                    attrs['class'] = 'tg-spoiler'
                else: return children_html # Ø§Ø³Ù¾ÙˆÛŒÙ„Ø± Ù†Ø¨ÙˆØ¯ØŒ ÙÙ‚Ø· Ù…Ø­ØªÙˆØ§
            elif tag_name == 'pre': # Ø¨Ø±Ø§ÛŒ preØŒ Ø§Ú¯Ø± code Ø¨Ø§ Ú©Ù„Ø§Ø³ Ø²Ø¨Ø§Ù† Ø¯Ø§Ø´Øª
                code_child = element.find('code', class_=lambda x: x and isinstance(x, list) and x[0].startswith('language-'))
                if code_child:
                    lang = html.escape(code_child['class'][0].split('-',1)[1], quote=True)
                    # Ù…Ø­ØªÙˆØ§ÛŒ Ú©Ø¯ Ø§Ø² Ù‚Ø¨Ù„ ØªÙˆØ³Ø· ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ escape Ø´Ø¯Ù‡ Ø§Ø³Øª
                    # Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¯Ù‚Øª Ø¯Ø§Ø±Ø¯ ØªØ§ Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒÙ… Ù…Ø­ØªÙˆØ§ÛŒ code_child ÙÙ‚Ø· Ù…ØªÙ† escape Ø´Ø¯Ù‡ Ø§Ø³Øª
                    # children_html Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ø§ÛŒØ¯ Ù…Ø­ØªÙˆØ§ÛŒ Ø¯Ø§Ø®Ù„ <code> Ø¨Ø§Ø´Ø¯.
                    return f'<pre><code class="language-{lang}">{children_html}</code></pre>'
                return f"<pre>{children_html}</pre>" # pre Ø³Ø§Ø¯Ù‡
            elif tag_name == 'code' and element.parent.name == 'pre':
                return children_html # Ù…Ø­ØªÙˆØ§ÛŒ code Ø¯Ø§Ø®Ù„ preØŒ Ø®ÙˆØ¯ pre ØªÚ¯ Ø±Ø§ Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯

            # Ø³Ø§Ø®Øª Ø±Ø´ØªÙ‡ Ø§ØªØ±ÛŒØ¨ÛŒÙˆØªâ€ŒÙ‡Ø§
            attrs_str = "".join([f' {k}="{v}"' for k, v in attrs.items()])
            return f"<{tag_name}{attrs_str}>{children_html}</{tag_name}>"
        
        # Ø§Ú¯Ø± ØªÚ¯ Ø¯Ø± Ù„ÛŒØ³Øª Ù…Ø¬Ø§Ø² Ù†Ø¨ÙˆØ¯ØŒ ÙÙ‚Ø· Ù…Ø­ØªÙˆØ§ÛŒ ÙØ±Ø²Ù†Ø¯Ø§Ù† (Ø­Ø°Ù ØªÚ¯)
        return children_html

    def _prepare_description_telegram_html(self, html_content: str) -> str:
        if not html_content: return ""
        soup = BeautifulSoup(html_content, "html.parser")

        # 1. Ø­Ø°Ù Ø¨Ø®Ø´ "Forwarded From"
        first_p = soup.find('p', recursive=False) # ÙÙ‚Ø· Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ø³Ø·Ø­ Ø§ÙˆÙ„
        if first_p and first_p.get_text(strip=True).lower().startswith("forwarded from"):
            first_p.decompose()
        
        # 2. ØªØ¨Ø¯ÛŒÙ„ Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ù‡ HTML Ø³Ø§Ø²Ú¯Ø§Ø± Ø¨Ø§ ØªÙ„Ú¯Ø±Ø§Ù…
        # Ù…Ø§ Ø¨Ù‡ Ø¬Ø§ÛŒ Ù¾ÛŒÙ…Ø§ÛŒØ´ Ø¯Ø³ØªÛŒ ØªÚ¯â€ŒÙ‡Ø§ÛŒ Ø¨Ù„Ø§Ú©ØŒ Ú©Ù„ Ù…Ø­ØªÙˆØ§ÛŒ soup (Ù¾Ø³ Ø§Ø² Ø­Ø°Ù Ø§Ø­ØªÙ…Ø§Ù„ÛŒ Forwarded From)
        # Ø±Ø§ Ø¨Ù‡ ØªØ§Ø¨Ø¹ Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…. ØªØ§Ø¨Ø¹ Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ <br> Ø±Ø§ Ø¨Ù‡ \n ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        # Ø³Ù¾Ø³ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø®Ø·ÙˆØ· Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§ÙÛŒ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯.
        
        processed_html_parts = []
        # Ø§Ú¯Ø± soup.body ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´Øª Ùˆ ÙØ±Ø²Ù†Ø¯ Ø¯Ø§Ø´ØªØŒ Ø§Ø² Ø¢Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ØŒ Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ†ØµÙˆØ±Øª Ø§Ø² Ø®ÙˆØ¯ soup
        container_to_process = soup.body if soup.body and soup.body.contents else soup
        
        for element in container_to_process.children: # Ù¾ÛŒÙ…Ø§ÛŒØ´ ÙØ±Ø²Ù†Ø¯Ø§Ù† Ø³Ø·Ø­ Ø§ÙˆÙ„ Ú©Ø§Ù†ØªÛŒÙ†Ø±
            html_part = self._recursive_html_to_telegram_html(element)
            # Ø§Ú¯Ø± Ø®ÙˆØ¯ element ÛŒÚ© Ø¨Ù„Ø§Ú© Ø§ØµÙ„ÛŒ Ø¨ÙˆØ¯ (Ù…Ø«Ù„ p, div)ØŒ Ø¨Ø¹Ø¯Ø´ ÛŒÚ© \n\n Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            # Ø§ÛŒÙ† Ø¨Ø±Ø§ÛŒ Ø­ÙØ¸ ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§ÙÛŒ Ø§Ø³ØªØŒ Ú†ÙˆÙ† ØªÙ„Ú¯Ø±Ø§Ù… <p> Ø±Ø§ Ø¨Ø±Ø§ÛŒ ÙØ§ØµÙ„Ù‡ Ù†Ù…ÛŒâ€ŒØ´Ù†Ø§Ø³Ø¯.
            if element.name in ['p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'ul', 'ol', 'blockquote', 'hr', 'table']:
                # Ø§Ú¯Ø± html_part Ø¨Ø§ \n ØªÙ…Ø§Ù… Ù†Ø´Ø¯Ù‡ Ø¨ÙˆØ¯ØŒ ÛŒÚ© \n Ø§Ø¶Ø§ÙÙ‡ Ú©Ù† (Ø¨Ø±Ø§ÛŒ Ø¬Ø¯Ø§Ø³Ø§Ø²ÛŒ Ø§Ø² Ø¨Ù„Ø§Ú© Ø¨Ø¹Ø¯ÛŒ)
                # Ùˆ Ú†ÙˆÙ† Ø¨Ù„Ø§Ú© Ø§Ø³ØªØŒ ÛŒÚ© \n Ø¯ÛŒÚ¯Ø± Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù
                stripped_part = html_part.rstrip('\n')
                if stripped_part: # ÙÙ‚Ø· Ø§Ú¯Ø± Ù…Ø­ØªÙˆØ§ Ø¯Ø§Ø´Øª
                    processed_html_parts.append(stripped_part + "\n\n")
            elif html_part.strip(): # Ø§Ú¯Ø± inline Ø¨ÙˆØ¯ ÛŒØ§ ÙÙ‚Ø· Ù…ØªÙ†
                processed_html_parts.append(html_part)
        
        final_html = "".join(processed_html_parts)
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø®Ø·ÙˆØ· Ø¬Ø¯ÛŒØ¯: Ø¨ÛŒØ´ Ø§Ø² Ø¯Ùˆ \n Ù…ØªÙˆØ§Ù„ÛŒ Ø±Ø§ Ø¨Ù‡ Ø¯Ùˆ \n ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†
        final_html = re.sub(r'\n\s*\n\s*\n+', '\n\n', final_html).strip()
        return final_html

    def format_event_message(self, event: EventInfo) -> str:
        display_title_text = event.title.strip() # Ø¹Ù†ÙˆØ§Ù† Ø¨Ù‡ ØµÙˆØ±Øª Ù…ØªÙ† Ø³Ø§Ø¯Ù‡
        
        # ØªÙˆØ¶ÛŒØ­Ø§Øª Ø¨Ù‡ ÙØ±Ù…Øª HTML Ø³Ø§Ø²Ú¯Ø§Ø± Ø¨Ø§ ØªÙ„Ú¯Ø±Ø§Ù… ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒØ´ÙˆØ¯
        description_telegram_html = self._prepare_description_telegram_html(event.description)
        
        # Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ ØªÚ©Ø±Ø§Ø±ØŒ ÛŒÚ© Ù†Ø³Ø®Ù‡ Ù…ØªÙ†ÛŒ Ø³Ø§Ø¯Ù‡ Ø§Ø² ØªÙˆØ¶ÛŒØ­Ø§Øª HTML Ø´Ø¯Ù‡ Ù„Ø§Ø²Ù… Ø¯Ø§Ø±ÛŒÙ…
        temp_soup_for_plain_text = BeautifulSoup(description_telegram_html, "html.parser")
        description_plain_text_for_check = temp_soup_for_plain_text.get_text(separator=' ', strip=True)

        show_separate_title = True # Ø¨Ù‡ Ø·ÙˆØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¹Ù†ÙˆØ§Ù† Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯

        if description_plain_text_for_check and display_title_text:
            leading_symbols_pattern = r"^[ğŸ”ğŸ–¼âšœï¸ğŸ“ğŸ“¢âœ”ï¸âœ…ğŸ”†ğŸ—“ï¸ğŸ“ğŸ’³#Ùªâ™¦ï¸ğŸ”¹ğŸ”¸ğŸŸ¢â™¦ï¸â–ªï¸â–«ï¸â–ªï¸â€¢â—ğŸ”˜ğŸ‘â€ğŸ—¨\s]*(?=[^\s])"
            trailing_punctuation_pattern = r"[\s.:â€¦]+$"
            def normalize_text(text):
                if not text: return ""
                text = re.sub(leading_symbols_pattern, "", text, flags=re.IGNORECASE).strip()
                text = re.sub(trailing_punctuation_pattern, "", text)
                return text.lower().strip()

            title_comp = normalize_text(display_title_text)
            if title_comp:
                first_desc_line_plain = description_plain_text_for_check.split('\n', 1)[0].strip()
                first_desc_line_comp = normalize_text(first_desc_line_plain)

                if first_desc_line_comp:
                    # Ø§Ú¯Ø± Ø¹Ù†ÙˆØ§Ù† Ù†Ø±Ù…Ø§Ù„â€ŒØ´Ø¯Ù‡ Ø¨Ø§ Ø®Ø· Ø§ÙˆÙ„ ØªÙˆØ¶ÛŒØ­Ø§Øª Ù†Ø±Ù…Ø§Ù„â€ŒØ´Ø¯Ù‡ ÛŒÚ©ÛŒ Ø¨ÙˆØ¯ØŒ ÛŒØ§ Ø´Ø¨Ø§Ù‡Øª Ø²ÛŒØ§Ø¯ÛŒ Ø¯Ø§Ø´Øª
                    if (title_comp == first_desc_line_comp) or \
                       (len(title_comp) > 10 and first_desc_line_comp.startswith(title_comp)) or \
                       (len(first_desc_line_comp) > 10 and title_comp.startswith(first_desc_line_comp)):
                        logger.info(f"Title ('{display_title_text}') considered part of description. Not showing separate title.")
                        show_separate_title = False # Ø¹Ù†ÙˆØ§Ù† Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù†Ø´ÙˆØ¯

        # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø·ÙˆÙ„ ØªÙˆØ¶ÛŒØ­Ø§Øª HTML (Ø¨Ø³ÛŒØ§Ø± Ø³Ø®Øª Ø§Ø³Øª Ú©Ù‡ Ø¨Ø¯ÙˆÙ† Ø´Ú©Ø³ØªÙ† HTML Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯)
        # ÙØ¹Ù„Ø§ Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø±Ø§ Ø³Ø§Ø¯Ù‡ Ù†Ú¯Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ø±ÛŒÙ… Ùˆ ÙÙ‚Ø· Ù‡Ø´Ø¯Ø§Ø± Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ… Ø§Ú¯Ø± Ø®ÛŒÙ„ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø¨ÙˆØ¯
        # DESCRIPTION_MAX_LEN_HTML = 3800 (Ø¯Ø± Ù¾Ø§Ø³Ø® Ù‚Ø¨Ù„ÛŒ Ø¨ÙˆØ¯ØŒ ÙØ¹Ù„Ø§ Ø­Ø°Ù Ø´Ø¯ ØªØ§ ØªØ³Øª Ú©Ù†ÛŒÙ…)
        
        if not description_telegram_html.strip(): # Ø§Ú¯Ø± ØªÙˆØ¶ÛŒØ­Ø§Øª Ø®Ø§Ù„ÛŒ Ø´Ø¯
            description_telegram_html = ""

        # Ù…ÙˆÙ†ØªØ§Ú˜ Ù¾ÛŒØ§Ù… Ø¨Ø§ HTML
        message_parts = []
        if show_separate_title and display_title_text:
             # RLM Ø¨Ø±Ø§ÛŒ Ø¹Ù†ÙˆØ§Ù† Ø§Ú¯Ø± Ø¨Ø§ Ú©Ø§Ø±Ø§Ú©ØªØ± LTR Ø´Ø±ÙˆØ¹ Ø´ÙˆØ¯
             title_prefix = self.RLM if display_title_text and not re.match(r"^\s*[\u0600-\u06FF]", display_title_text) else ""
             message_parts.append(f"{title_prefix}<b>{html.escape(display_title_text)}</b>")

        if description_telegram_html:
            separator = "\n\n" if message_parts else "" # Ø§Ú¯Ø± Ø¹Ù†ÙˆØ§Ù† Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯ØŒ Ø¯Ùˆ Ø®Ø· ÙØ§ØµÙ„Ù‡
            # Ø®ÙˆØ¯ description_telegram_html Ø¨Ø§ÛŒØ¯ Ø´Ø§Ù…Ù„ RLM Ù‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø¨Ø§Ø´Ø¯ Ø§Ú¯Ø± Ø§Ø² _recursive_html_to_telegram_html Ù…ÛŒâ€ŒØ¢ÛŒØ¯
            # ÛŒØ§ Ø§ÛŒÙ†Ú©Ù‡ Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ÛŒ Ù‡Ø± Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ø§ØµÙ„ÛŒ (Ú©Ù‡ Ø¨Ø§ \n\n Ø¬Ø¯Ø§ Ø´Ø¯Ù‡) RLM Ø¨Ú¯Ø°Ø§Ø±ÛŒÙ…
            # ÙØ¹Ù„Ø§ RLM Ú©Ù„ÛŒ Ø¨Ø±Ø§ÛŒ ØªÙˆØ¶ÛŒØ­Ø§Øª Ù†Ù…ÛŒâ€ŒÚ¯Ø°Ø§Ø±ÛŒÙ…ØŒ Ø¨Ù‡ HTML ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø§ØªÚ©Ø§ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            message_parts.append(f"{separator}{description_telegram_html}")

        meta_info_parts = []
        escaped_link_text = html.escape("Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ø§Ù…Ù„ Ø±ÙˆÛŒØ¯Ø§Ø¯")
        if event.link:
            meta_info_parts.append(f"{self.RLM}ğŸ”— <a href=\"{self._sanitize_href(event.link)}\">{escaped_link_text}</a>")
        
        escaped_source_label = html.escape("Ù…Ù†Ø¨Ø¹:")
        escaped_source_name = html.escape(event.source_channel)
        if event.source_channel_username:
            escaped_username = html.escape(event.source_channel_username)
            meta_info_parts.append(f"{self.RLM}ğŸ“¢ <b>{escaped_source_label}</b> <a href=\"https://t.me/{escaped_username}\">{escaped_source_name}</a>")
        else:
            meta_info_parts.append(f"{self.RLM}ğŸ“¢ <b>{escaped_source_label}</b> {escaped_source_name}")

        if event.published:
            formatted_date = event.published
            try:
                date_obj = datetime.strptime(event.published, "%a, %d %b %Y %H:%M:%S %Z")
                formatted_date = date_obj.strftime("%d %b %Y - %H:%M (%Z)")
            except: pass 
            if formatted_date:
                 meta_info_parts.append(f"{self.RLM}ğŸ“… <b>{html.escape('Ø§Ù†ØªØ´Ø§Ø±:')}</b> {html.escape(formatted_date)}")

        if meta_info_parts:
            separator_meta = "\n\n" if message_parts else "" 
            message_parts.append(separator_meta + "\n".join(meta_info_parts))

        final_message_html = "\n".join(message_parts).strip()
                                                          
        TELEGRAM_MSG_MAX_LEN = 4096
        if len(final_message_html) > TELEGRAM_MSG_MAX_LEN:
            logger.warning(f"HTML Message for '{display_title_text}' too long ({len(final_message_html)} chars). Telegram might truncate or reject.")
            # Ø±Ø§Ù‡ Ø­Ù„ Ø³Ø§Ø¯Ù‡: Ú©ÙˆØªØ§Ù‡ Ú©Ø±Ø¯Ù† Ø§Ø² Ø§Ù†ØªÙ‡Ø§ (Ù…Ù…Ú©Ù† Ø§Ø³Øª HTML Ø±Ø§ Ø¨Ø´Ú©Ù†Ø¯)
            # final_message_html = final_message_html[:TELEGRAM_MSG_MAX_LEN - 20] + "..." # Ø§ÛŒÙ† Ø®Ø·Ø±Ù†Ø§Ú© Ø§Ø³Øª Ø¨Ø±Ø§ÛŒ HTML
            # Ø¨Ù‡ØªØ± Ø§Ø³Øª Ø§Ø² Ø§Ø¨ØªØ¯Ø§ ØªÙˆØ¶ÛŒØ­Ø§Øª Ø±Ø§ Ú©ÙˆØªØ§Ù‡ Ú©Ù†ÛŒÙ… Ø§Ú¯Ø± Ù„Ø§Ø²Ù… Ø´Ø¯.
            
        return final_message_html

    async def publish_event(self, event: EventInfo):
        # (Ø§ÛŒÙ† Ù…ØªØ¯ Ù‡Ù…Ø§Ù†Ù†Ø¯ Ø¢Ø®Ø±ÛŒÙ† Ù†Ø³Ø®Ù‡ Ú©Ø§Ù…Ù„ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ø¨Ø§Ù‚ÛŒ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯ØŒ ÙÙ‚Ø· parse_mode='HTML' Ù…ÛŒâ€ŒØ´ÙˆØ¯)
        try:
            message_html = self.format_event_message(event)
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ Ù¾ÛŒØ§Ù… ÙˆØ§Ù‚Ø¹Ø§ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª (Ø­ØªÛŒ Ø§Ú¯Ø± ÙÙ‚Ø· Ø´Ø§Ù…Ù„ ØªÚ¯â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ ÛŒØ§ RLM Ø¨Ø§Ø´Ø¯)
            temp_soup = BeautifulSoup(message_html, "html.parser")
            is_message_effectively_empty = not temp_soup.get_text(strip=True)

            if is_message_effectively_empty :
                logger.info(f"Skipping due to effectively empty HTML message for event from {event.source_channel} (Title: {event.title[:30]}...).")
                return

            await self.bot.send_message(
                chat_id=self.target_channel, text=message_html,
                parse_mode=ParseMode.HTML, # <--- ØªØºÛŒÛŒØ± Ø¨Ù‡ ParseMode.HTML
                disable_web_page_preview=True 
            )
            logger.info(f"Published event (HTML): {event.title[:60]}... from {event.source_channel}")
        except Exception as e:
            logger.error(f"Failed to publish event ({event.title[:60]}...) using HTML mode: {e}", exc_info=True)

    async def run_monitoring_loop(self):
        # (Ø§ÛŒÙ† Ù…ØªØ¯ Ù‡Ù…Ø§Ù†Ù†Ø¯ Ø¢Ø®Ø±ÛŒÙ† Ù†Ø³Ø®Ù‡ Ú©Ø§Ù…Ù„ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ø¨Ø§Ù‚ÛŒ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯ØŒ Ø¨Ø§ check_interval_seconds=180)
        logger.info("Starting RSS monitoring...")
        await asyncio.sleep(10) 
        while True:
            logger.info("Checking all feeds...")
            all_new_events = []
            async with aiohttp.ClientSession() as session:
                tasks = [self.fetch_feed(session, feed_info) for feed_info in self.rss_feeds]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                for result in results:
                    if isinstance(result, list): all_new_events.extend(result)
                    elif isinstance(result, Exception): logger.error(f"Feed task error: {result}", exc_info=result)
            
            if all_new_events:
                logger.info(f"Found {len(all_new_events)} new event(s).")
                for event_to_publish in all_new_events:
                    await self.publish_event(event_to_publish)
                    await asyncio.sleep(5) 
            else:
                logger.info("No new events found.")
            
            check_interval_seconds = 180 
            logger.info(f"Next check in {check_interval_seconds // 60} minutes.")
            await asyncio.sleep(check_interval_seconds)


# ... (Ú©Ù„Ø§Ø³ Config Ùˆ ØªÙˆØ§Ø¨Ø¹ health_check, start_web_server, main Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø§Ø² Ù¾Ø§Ø³Ø® Ù‚Ø¨Ù„ÛŒ) ...
class Config:
    BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')
    TARGET_CHANNEL = os.getenv('TARGET_CHANNEL')

rss_bot_instance: Optional[RSSTelegramBot] = None 

async def health_check(request):
    global rss_bot_instance
    if rss_bot_instance:
        return web.Response(text=f"Bot is running! Processed items: {len(rss_bot_instance.processed_items)}", status=200)
    return web.Response(text="Bot not fully initialized.", status=200)

async def start_web_server():
    app = web.Application()
    app.router.add_get('/health', health_check)
    app.router.add_get('/', health_check)
    runner = web.AppRunner(app)
    await runner.setup()
    port = int(os.environ.get("PORT", "7860"))
    site = web.TCPSite(runner, '0.0.0.0', port)
    try:
        await site.start()
        logger.info(f"Web server started on port {port}")
    except OSError as e:
        logger.error(f"Failed to start web server on port {port}: {e}.")

async def main():
    global rss_bot_instance
    if not Config.BOT_TOKEN or not Config.TARGET_CHANNEL:
        logger.critical("Missing TELEGRAM_BOT_TOKEN or TARGET_CHANNEL environment variables!")
        return
    
    logger.info("Application starting...")
    rss_bot_instance = RSSTelegramBot(bot_token=Config.BOT_TOKEN, target_channel=Config.TARGET_CHANNEL)
    try:
        bot_info = await rss_bot_instance.bot.get_me()
        logger.info(f"Bot started: @{bot_info.username}")
        web_server_task = asyncio.create_task(start_web_server())
        await rss_bot_instance.run_monitoring_loop()
    except KeyboardInterrupt:
        logger.info("Bot stopped by user.")
    except Exception as e:
        logger.error(f"Critical bot error in main: {e}", exc_info=True)
    finally:
        logger.info("Bot shutting down...")
        if 'web_server_task' in locals() and web_server_task and not web_server_task.done(): 
            web_server_task.cancel()
            try: await web_server_task
            except asyncio.CancelledError: logger.info("Web server task cancelled.")
        logger.info("Application fully stopped.")

if __name__ == "__main__":
    asyncio.run(main())
