import asyncio
import re
import logging
from datetime import datetime
import aiohttp
import feedparser
from telegram import Bot
import os
from dataclasses import dataclass
from typing import List, Optional
from bs4 import BeautifulSoup # <--- Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ¬Ø²ÛŒÙ‡ HTML
from aiohttp import web

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class EventInfo:
    title: str
    description: str  # HTML Ø®Ø§Ù… Ø§Ø² ÙÛŒØ¯
    link: str
    published: str
    source_channel: str
    source_channel_username: Optional[str] = None


class EventDetector:
    EVENT_KEYWORDS = [
        'ÙˆØ¨ÛŒÙ†Ø§Ø±', 'webinar', 'Ú©Ø§Ø±Ú¯Ø§Ù‡', 'workshop', 'Ø³Ù…ÛŒÙ†Ø§Ø±', 'seminar', 'Ú©Ù†ÙØ±Ø§Ù†Ø³', 'conference', 
        'Ù‡Ù…Ø§ÛŒØ´', 'congress', 'Ù†Ø´Ø³Øª', 'meeting', 'Ø¯ÙˆØ±Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ', 'course', 'Ú©Ù„Ø§Ø³', 'class', 
        'Ø§ÛŒÙˆÙ†Øª', 'event', 'Ø¨Ø±Ú¯Ø²Ø§Ø±', 'organize', 'Ø´Ø±Ú©Øª', 'participate', 'Ø«Ø¨Øª Ù†Ø§Ù…', 'register',
        'Ø±Ø§ÛŒÚ¯Ø§Ù†', 'free', 'Ø¢Ù†Ù„Ø§ÛŒÙ†', 'online', 'Ù…Ø¬Ø§Ø²ÛŒ', 'virtual', 'Ø¢Ù…ÙˆØ²Ø´', 'training', 
        'ÙØ±Ø§Ø®ÙˆØ§Ù†', 'call', 'Ú¯ÙˆØ§Ù‡ÛŒ', 'certificate', 'Ù…Ø¯Ø±Ú©', 'certification', 'Ù„Ø§ÛŒÙˆ', 'live'
    ]

    def detect_event(self, title: str, description_html: str) -> bool:
        soup = BeautifulSoup(description_html, "html.parser")
        # Ø­Ø°Ù Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø®Ø´ "Forwarded From" Ø¨Ø±Ø§ÛŒ Ø¯Ù‚Øª Ø¨ÛŒØ´ØªØ± Ø¯Ø± ØªØ´Ø®ÛŒØµ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
        first_p_tag = soup.find('p')
        if first_p_tag and first_p_tag.get_text(strip=True).lower().startswith("forwarded from"):
            first_p_tag.decompose()
        
        text_only_description = soup.get_text(separator=' ', strip=True)
        full_text = f"{title} {text_only_description}"
        text_lower = full_text.lower()

        matches = sum(1 for keyword in self.EVENT_KEYWORDS if keyword in text_lower)
        has_specific_pattern = any([
            'Ø«Ø¨Øª Ù†Ø§Ù…' in text_lower, 'Ø´Ø±Ú©Øª Ø¯Ø±' in text_lower, 
            # 'Ø¨Ø±Ú¯Ø²Ø§Ø± Ù…ÛŒ' in text_lower, # Ø§ÛŒÙ† Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨ÛŒØ´ Ø§Ø² Ø­Ø¯ Ú©Ù„ÛŒ Ø¨Ø§Ø´Ø¯
            'register' in text_lower, 'join' in text_lower, 'ÙˆØ¨ÛŒÙ†Ø§Ø±' in text_lower,
            'Ú©Ø§Ø±Ú¯Ø§Ù‡' in text_lower, 'Ø¯ÙˆØ±Ù‡ Ø¢Ù†Ù„Ø§ÛŒÙ†' in text_lower, 'Ø³Ù…ÛŒÙ†Ø§Ø±' in text_lower
        ])
        # Ø§Ú¯Ø± Ø¹Ù†ÙˆØ§Ù† Ø´Ø§Ù…Ù„ Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ Ø¨ÙˆØ¯ ÛŒØ§ ØªÙˆØ¶ÛŒØ­Ø§Øª Ø´Ø§Ù…Ù„ Ø§Ù„Ú¯ÙˆÛŒ Ø®Ø§Øµ Ø¨ÙˆØ¯ ÛŒØ§ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ú©Ø§ÙÛŒ Ø¨ÙˆØ¯
        title_has_keyword = any(keyword in title.lower() for keyword in ['ÙˆØ¨ÛŒÙ†Ø§Ø±', 'Ú©Ø§Ø±Ú¯Ø§Ù‡', 'Ø³Ù…ÛŒÙ†Ø§Ø±', 'Ø¯ÙˆØ±Ù‡', 'Ú©Ù†ÙØ±Ø§Ù†Ø³', 'Ù‡Ù…Ø§ÛŒØ´'])
        
        return title_has_keyword or has_specific_pattern or matches >= 1


class RSSTelegramBot:
    def __init__(self, bot_token: str, target_channel: str):
        self.bot_token = bot_token
        self.target_channel = target_channel
        self.bot = Bot(token=bot_token)
        self.detector = EventDetector()
        self.processed_items = set()
        self.rss_feeds = [
            {'name': 'WinCell Co', 'url': 'https://rsshub.app/telegram/channel/wincellco', 'channel': 'wincellco'},
            {'name': 'Rayazistazma', 'url': 'https://rsshub.app/telegram/channel/Rayazistazma', 'channel': 'Rayazistazma'},
            {'name': 'SBU Bio Society', 'url': 'https://rsshub.app/telegram/channel/SBUBIOSOCIETY', 'channel': 'SBUBIOSOCIETY'},
            {'name': 'Test BioPy Channel', 'url': 'https://rsshub.app/telegram/channel/testbiopy', 'channel': 'testbiopy'}
        ]

    async def fetch_feed(self, session: aiohttp.ClientSession, feed_info: dict) -> List[EventInfo]:
        # (Ø§ÛŒÙ† Ù…ØªØ¯ Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø¢Ø®Ø±ÛŒÙ† Ù†Ø³Ø®Ù‡ Ú©Ø§Ù…Ù„ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ø¨Ø§Ù‚ÛŒ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯ØŒ Ù…Ú¯Ø± Ø§ÛŒÙ†Ú©Ù‡ Ø¨Ø®ÙˆØ§Ù‡ÛŒØ¯ ØªØ¹Ø¯Ø§Ø¯ feed.entries[:X] Ø±Ø§ ØªØºÛŒÛŒØ± Ø¯Ù‡ÛŒØ¯)
        # ... (Ú©Ø¯ fetch_feed Ø§Ø² Ù¾Ø§Ø³Ø® Ù‚Ø¨Ù„ÛŒ Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ Ú©Ù¾ÛŒ Ú©Ù†ÛŒØ¯) ...
        # ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†ØŒ Ø¨Ø®Ø´ Ø§ØµÙ„ÛŒ Ø¢Ù† Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ Ù…ÛŒ Ø¢ÙˆØ±Ù…:
        events = []
        feed_url, feed_name = feed_info['url'], feed_info['name']
        logger.info(f"Fetching feed: {feed_name} from {feed_url}")
        try:
            async with session.get(feed_url, timeout=45) as response:
                if response.status == 200:
                    content = await response.text()
                    feed = feedparser.parse(content)
                    logger.info(f"Fetched {feed_name}. Entries: {len(feed.entries)}")
                    for entry in feed.entries[:10]: # ÛŒØ§ :5 Ø§Ú¯Ø± Ù‡Ù†ÙˆØ² Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯
                        entry_id = f"{feed_info.get('channel', feed_name)}_{entry.get('id', entry.get('link', ''))}"
                        if entry_id not in self.processed_items:
                            raw_title = entry.get('title', '').strip()
                            raw_description_html = entry.get('description', entry.get('summary', ''))
                            if self.detector.detect_event(raw_title, raw_description_html):
                                event = EventInfo(
                                    title=raw_title, description=raw_description_html,
                                    link=entry.get('link', ''), published=entry.get('published', ''),
                                    source_channel=feed_name, source_channel_username=feed_info.get('channel')
                                )
                                events.append(event)
                                self.processed_items.add(entry_id)
                        if len(self.processed_items) > 1500:
                            self.processed_items = set(list(self.processed_items)[500:])
                            logger.info(f"Cleaned up processed_items. New size: {len(self.processed_items)}")
                else:
                    logger.error(f"Error fetching {feed_name}: Status {response.status} - {await response.text(encoding='utf-8', errors='ignore')}")
        except Exception as e:
            logger.error(f"Exception in fetch_feed for {feed_name} ({feed_url}): {e}", exc_info=True)
        return events


    def _clean_html_and_extract_text(self, html_content: str) -> str:
        """
        HTML Ø±Ø§ Ø¨Ø§ BeautifulSoup Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§ Ù…ØªÙ† Ø³Ø§Ø¯Ù‡ Ø¨Ø§ ÙØ±Ù…Øª Ø®ÙˆØ¨ ØªÙˆÙ„ÛŒØ¯ Ø´ÙˆØ¯.
        - Ø¨Ø®Ø´ "Forwarded From" Ø±Ø§ Ø­Ø°Ù Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        - ØªÚ¯ <br> Ø±Ø§ Ø¨Ù‡ Ø®Ø· Ø¬Ø¯ÛŒØ¯ ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        - Ø³Ø¹ÛŒ Ø¯Ø± Ø­ÙØ¸ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø§Ø±Ø¯.
        """
        if not html_content:
            return ""

        soup = BeautifulSoup(html_content, "html.parser")

        # 1. Ø­Ø°Ù Ø¨Ø®Ø´ "Forwarded From"
        # Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø§ÙˆÙ„ÛŒÙ† Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒ RSSHub Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ ÙÙˆØ±ÙˆØ§Ø±Ø¯ Ø´Ø¯Ù‡ Ø§Ø³Øª.
        first_p_tag = soup.find('p')
        if first_p_tag:
            first_p_text = first_p_tag.get_text(strip=True)
            if first_p_text.lower().startswith("forwarded from"):
                logger.debug(f"Removing 'Forwarded From' paragraph: {first_p_tag.get_text(strip=True)[:100]}")
                first_p_tag.decompose() # Ø­Ø°Ù Ú©Ø§Ù…Ù„ ØªÚ¯ p Ùˆ Ù…Ø­ØªÙˆÛŒØ§ØªØ´

        # 2. ØªØ¨Ø¯ÛŒÙ„ ØªÚ¯â€ŒÙ‡Ø§ÛŒ <br> Ø¨Ù‡ Ú©Ø§Ø±Ø§Ú©ØªØ± Ø®Ø· Ø¬Ø¯ÛŒØ¯ (\n)
        for br_tag in soup.find_all("br"):
            br_tag.replace_with("\n")

        # 3. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø¨Ø§ Ø­ÙØ¸ Ø³Ø§Ø®ØªØ§Ø± Ø¨Ù‡ØªØ±
        # Ø§Ø² get_text(separator='\n') Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú©Ù‡ Ø¨ÛŒÙ† Ø¨Ù„Ø§Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø®Ø· Ø¬Ø¯ÛŒØ¯ Ù…ÛŒâ€ŒÚ¯Ø°Ø§Ø±Ø¯
        # Ùˆ strip=True ÙØ¶Ø§Ù‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ Ø§Ø¶Ø§ÙÛŒ Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ Ùˆ Ø§Ù†ØªÙ‡Ø§ÛŒ Ù‡Ø± Ø¨Ø®Ø´ Ù…ØªÙ†ÛŒ Ø±Ø§ Ø­Ø°Ù Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        text_content = soup.get_text(separator='\n', strip=True)
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø´Ú©Ø³ØªÚ¯ÛŒ Ø®Ø·ÙˆØ·: Ø­Ø°Ù Ø®Ø·ÙˆØ· Ú©Ø§Ù…Ù„Ø§Ù‹ Ø®Ø§Ù„ÛŒ Ùˆ ÙØ¶Ø§Ù‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ Ø§Ø¨ØªØ¯Ø§/Ø§Ù†ØªÙ‡Ø§ÛŒ Ù‡Ø± Ø®Ø·
        lines = [line.strip() for line in text_content.splitlines()]
        cleaned_text = "\n".join(line for line in lines if line) # ÙÙ‚Ø· Ø®Ø·ÙˆØ· ØºÛŒØ±Ø®Ø§Ù„ÛŒ Ø±Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø±

        return cleaned_text

    def format_event_message(self, event: EventInfo) -> str:
        RLM = "\u200F"

        display_title = event.title.strip()
        
        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ ØªÙˆØ¶ÛŒØ­Ø§Øª Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…ØªØ¯ Ø¬Ø¯ÛŒØ¯
        description_cleaned_text = self._clean_html_and_extract_text(event.description)
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¹Ù†ÙˆØ§Ù† Ùˆ ØªÙˆØ¶ÛŒØ­Ø§Øª Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ ØªÚ©Ø±Ø§Ø±
        # ÛŒÚ© Ù„ÛŒØ³Øª Ø§Ø² Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§/Ø§ÛŒÙ…ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ Ø¨Ø§Ø´Ù†Ø¯ Ùˆ Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ÛŒØ¯ Ø­Ø°Ù Ø´ÙˆÙ†Ø¯
        leading_chars_to_strip_pattern = r"^[ğŸ”ğŸ–¼âšœï¸ğŸ“ğŸ“¢âœ”ï¸âœ…ğŸ”†ğŸ—“ï¸ğŸ“ğŸ’³#Ùªâ™¦ï¸ğŸ”¹ğŸ”¸ğŸŸ¢â™¦ï¸â–ªï¸â–«ï¸â–ªï¸â€¢â—ğŸ”˜ğŸ‘â€ğŸ—¨\s]*(?=[^\s])"
        # Ø­Ø°Ù Ø§ÛŒÙ† Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ Ø§Ø² Ø§Ø¨ØªØ¯Ø§ÛŒ Ø¹Ù†ÙˆØ§Ù† Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡
        normalized_title_for_comparison = re.sub(leading_chars_to_strip_pattern, "", display_title, flags=re.IGNORECASE).strip().lower()
        normalized_title_for_comparison = re.sub(r"[\s.:â€¦]+$", "", normalized_title_for_comparison) # Ø­Ø°Ù Ù†Ù‚Ø·Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø§Ù†ØªÙ‡Ø§ÛŒÛŒ

        description_to_display = description_cleaned_text
        title_is_separate = True # Ø¨Ù‡ Ø·ÙˆØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¹Ù†ÙˆØ§Ù† Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯

        if description_cleaned_text and normalized_title_for_comparison:
            first_desc_line = description_cleaned_text.split('\n', 1)[0].strip()
            # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø®Ø· Ø§ÙˆÙ„ ØªÙˆØ¶ÛŒØ­Ø§Øª Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡
            normalized_first_desc_line = re.sub(leading_chars_to_strip_pattern, "", first_desc_line, flags=re.IGNORECASE).strip().lower()
            normalized_first_desc_line = re.sub(r"[\s.:â€¦]+$", "", normalized_first_desc_line)

            # Ø§Ú¯Ø± Ø¹Ù†ÙˆØ§Ù† Ù†Ø±Ù…Ø§Ù„â€ŒØ´Ø¯Ù‡ Ùˆ Ø®Ø· Ø§ÙˆÙ„ ØªÙˆØ¶ÛŒØ­Ø§Øª Ù†Ø±Ù…Ø§Ù„â€ŒØ´Ø¯Ù‡ ÛŒÚ©ÛŒ Ø¨ÙˆØ¯Ù†Ø¯
            # ÛŒØ§ Ø§Ú¯Ø± Ø¹Ù†ÙˆØ§Ù† Ø¨Ø®Ø´ Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ÛŒ Ø§Ø² Ø§Ø¨ØªØ¯Ø§ÛŒ Ø®Ø· Ø§ÙˆÙ„ ØªÙˆØ¶ÛŒØ­Ø§Øª Ø¨ÙˆØ¯ (Ù…Ø«Ù„Ø§Ù‹ Ø¹Ù†ÙˆØ§Ù† Ú©ÙˆØªØ§Ù‡ Ø´Ø¯Ù‡ Ø¯Ø± RSS)
            if (normalized_title_for_comparison == normalized_first_desc_line) or \
               (len(normalized_title_for_comparison) > 8 and normalized_first_desc_line.startswith(normalized_title_for_comparison)) or \
               (len(normalized_first_desc_line) > 8 and normalized_title_for_comparison.startswith(normalized_first_desc_line)): # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ ÛŒÚ©ÛŒ Ù¾ÛŒØ´ÙˆÙ†Ø¯ Ø¯ÛŒÚ¯Ø±ÛŒ Ø§Ø³Øª
                
                logger.info(f"Title considered redundant or part of description's first line for: '{display_title}'")
                # Ø¯Ø± Ø§ÛŒÙ† Ø­Ø§Ù„ØªØŒ Ø¹Ù†ÙˆØ§Ù† Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ ØªÙˆØ¶ÛŒØ­Ø§Øª Ø§Ø² Ø§Ø¨ØªØ¯Ø§ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
                # (Ú†ÙˆÙ† Ø®ÙˆØ¯ ØªÙˆØ¶ÛŒØ­Ø§Øª Ø´Ø§Ù…Ù„ Ø®Ø· Ø§ÙˆÙ„ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø´Ø¨ÛŒÙ‡ Ø¹Ù†ÙˆØ§Ù† Ø§Ø³Øª)
                # ÛŒØ§ Ø§Ú¯Ø± Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ Ø­ØªÙ…Ø§ Ø¹Ù†ÙˆØ§Ù† Ø¬Ø¯Ø§ Ø¨Ø§Ø´Ø¯ Ùˆ Ø®Ø· Ø§ÙˆÙ„ Ø§Ø² ØªÙˆØ¶ÛŒØ­Ø§Øª Ø­Ø°Ù Ø´ÙˆØ¯:
                if '\n' in description_cleaned_text:
                    description_to_display = description_cleaned_text.split('\n', 1)[1].strip()
                else: # ØªÙˆØ¶ÛŒØ­Ø§Øª ÙÙ‚Ø· Ù‡Ù…Ø§Ù† ÛŒÚ© Ø®Ø· Ø¨ÙˆØ¯
                    description_to_display = "" 
                # ØªÙ…ÛŒØ²Ú©Ø§Ø±ÛŒ Ù…Ø¬Ø¯Ø¯ Ø§Ú¯Ø± Ø¨Ø§ Ø­Ø°Ù Ø®Ø· Ø§ÙˆÙ„ØŒ Ø®Ø§Ù„ÛŒ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
                description_to_display = "\n".join(filter(None, (line.strip() for line in description_to_display.splitlines())))
                
        # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø·ÙˆÙ„ Ù†Ù‡Ø§ÛŒÛŒ ØªÙˆØ¶ÛŒØ­Ø§Øª
        DESCRIPTION_MAX_LEN = 2000  # Ø§ÙØ²Ø§ÛŒØ´ Ø¨ÛŒØ´ØªØ± Ù…Ø­Ø¯ÙˆØ¯ÛŒØªØŒ Ú†ÙˆÙ† Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ Ú†ÛŒØ²ÛŒ Ø§Ø² Ù‚Ù„Ù… Ù†ÛŒÙØªØ¯
                                     # Ø§Ù…Ø§ Ù…Ø±Ø§Ù‚Ø¨ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ú©Ù„ÛŒ ØªÙ„Ú¯Ø±Ø§Ù… (Û´Û°Û¹Û¶ Ú©Ø§Ø±Ø§Ú©ØªØ±) Ø¨Ø§Ø´ÛŒØ¯
        if len(description_to_display) > DESCRIPTION_MAX_LEN:
            # Ø³Ø¹ÛŒ Ú©Ù† Ø¯Ø± ÛŒÚ© Ù†Ù‚Ø·Ù‡ Ù…Ù†Ø§Ø³Ø¨ (Ù…Ø«Ù„ Ø§Ù†ØªÙ‡Ø§ÛŒ Ø¬Ù…Ù„Ù‡) Ú©ÙˆØªØ§Ù‡ Ú©Ù†ÛŒ
            cut_off_point = description_to_display.rfind('.', 0, DESCRIPTION_MAX_LEN)
            if cut_off_point != -1 and cut_off_point > DESCRIPTION_MAX_LEN - 300: # Ø§Ú¯Ø± Ù†Ù‚Ø·Ù‡ Ø®ÛŒÙ„ÛŒ Ø¯ÙˆØ± Ù†Ø¨ÙˆØ¯
                 description_to_display = description_to_display[:cut_off_point+1] + f"{RLM} (...)"
            else:
                 description_to_display = description_to_display[:DESCRIPTION_MAX_LEN] + f"{RLM}..."
        
        if not description_to_display.strip(): # Ø§Ú¯Ø± ØªÙˆØ¶ÛŒØ­Ø§Øª Ø®Ø§Ù„ÛŒ Ø´Ø¯
            description_to_display = ""

        # Ù…ÙˆÙ†ØªØ§Ú˜ Ù¾ÛŒØ§Ù…
        message_parts = []
        if display_title: # Ø¹Ù†ÙˆØ§Ù† Ù‡Ù…ÛŒØ´Ù‡ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ (Ù…Ú¯Ø± Ø§ÛŒÙ†Ú©Ù‡ Ø¯Ø± Ø¢ÛŒÙ†Ø¯Ù‡ ØªØµÙ…ÛŒÙ… Ø¯ÛŒÚ¯Ø±ÛŒ Ø¨Ú¯ÛŒØ±ÛŒÙ…)
             message_parts.append(f"{RLM}ğŸ“ **{display_title}**")

        if description_to_display:
            # Ø§Ú¯Ø± Ø¹Ù†ÙˆØ§Ù† Ùˆ ØªÙˆØ¶ÛŒØ­Ø§Øª Ù‡Ø± Ø¯Ùˆ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ù†Ø¯ØŒ Ø¯Ùˆ Ø®Ø· ÙØ§ØµÙ„Ù‡
            # Ø§Ú¯Ø± ÙÙ‚Ø· ØªÙˆØ¶ÛŒØ­Ø§Øª ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ (Ùˆ Ø¹Ù†ÙˆØ§Ù† Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù†Ø´Ø¯Ù‡)ØŒ ÙØ§ØµÙ„Ù‡â€ŒØ§ÛŒ Ù„Ø§Ø²Ù… Ù†ÛŒØ³Øª
            separator = "\n\n" if display_title else ""
            message_parts.append(f"{separator}{RLM}{description_to_display}")

        meta_info_parts = []
        if event.link:
            meta_info_parts.append(f"{RLM}ğŸ”— [Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ø§Ù…Ù„ Ø±ÙˆÛŒØ¯Ø§Ø¯]({event.link})")
        
        if event.source_channel_username:
            meta_info_parts.append(f"{RLM}ğŸ“¢ **Ù…Ù†Ø¨Ø¹:** [{event.source_channel}](https://t.me/{event.source_channel_username})")
        else:
            meta_info_parts.append(f"{RLM}ğŸ“¢ **Ù…Ù†Ø¨Ø¹:** {event.source_channel}")

        if event.published:
            formatted_date = ""
            try:
                date_obj = datetime.strptime(event.published, "%a, %d %b %Y %H:%M:%S %Z") # Fri, 23 May 2025 22:41:11 GMT
                formatted_date = date_obj.strftime("%d %b %Y - %H:%M (%Z)") # 23 May 2025 - 22:41 (GMT)
            except ValueError:
                try: # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ ÙØ±Ù…Øª Ø³Ø§Ø¯Ù‡â€ŒØªØ±
                    main_date_part = event.published.split(',')[1].strip() if ',' in event.published else event.published
                    formatted_date = main_date_part.rsplit(':',2)[0] + " " + main_date_part.rsplit(' ',1)[-1] # 23 May 2025 22 (GMT)
                except: formatted_date = event.published # Ù†Ù…Ø§ÛŒØ´ Ø®Ø§Ù…
            if formatted_date:
                 meta_info_parts.append(f"{RLM}ğŸ“… **Ø§Ù†ØªØ´Ø§Ø±:** {formatted_date}")

        if meta_info_parts:
            # Ø§Ú¯Ø± Ø¨Ø®Ø´ Ø§ØµÙ„ÛŒ Ù¾ÛŒØ§Ù… (Ø¹Ù†ÙˆØ§Ù† ÛŒØ§ ØªÙˆØ¶ÛŒØ­Ø§Øª) ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªØŒ Ø¯Ùˆ Ø®Ø· ÙØ§ØµÙ„Ù‡ ØªØ§ Ù…ØªØ§
            separator_meta = "\n\n" if message_parts else ""
            message_parts.append(separator_meta + "\n".join(meta_info_parts))

        final_message = "\n".join(message_parts).strip() # filter(None,..) Ø­Ø°Ù Ø´Ø¯ ØªØ§ ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ø¯ÛŒ Ø­ÙØ¸ Ø´ÙˆÙ†Ø¯
                                                          # .strip() Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù ÙØ¶Ø§Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ Ú©Ù„ Ù¾ÛŒØ§Ù…
        
        TELEGRAM_MSG_MAX_LEN = 4096
        if len(final_message) > TELEGRAM_MSG_MAX_LEN:
            logger.warning(f"Message for '{display_title}' too long ({len(final_message)} chars), will be truncated by Telegram or cause error.")
            # Ø§Ú¯Ø± Ø®ÛŒÙ„ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø´Ø¯ØŒ Ø¨Ø§ÛŒØ¯ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø¨Ù‡ØªØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ú©ÙˆØªØ§Ù‡ Ú©Ø±Ø¯Ù† Ú©Ù„ Ù¾ÛŒØ§Ù… Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒÙ…
            # ÙØ¹Ù„Ø§ ÙÙ‚Ø· Ù‡Ø´Ø¯Ø§Ø± Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…. ØªÙ„Ú¯Ø±Ø§Ù… Ø®ÙˆØ¯Ø´ Ú©ÙˆØªØ§Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ÛŒØ§ Ø®Ø·Ø§ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.
            
        return final_message

    # ... (Ù…ØªØ¯Ù‡Ø§ÛŒ publish_event Ùˆ run_monitoring_loop Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø§Ø² Ù¾Ø§Ø³Ø® Ù‚Ø¨Ù„ÛŒ) ...
    async def publish_event(self, event: EventInfo):
        try:
            message = self.format_event_message(event)
            if not message or (not event.title and not event.description): # Ø§Ú¯Ø± Ù¾ÛŒØ§Ù… ÛŒØ§ Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯
                logger.info(f"Skipping empty or content-less message for an event from {event.source_channel}.")
                return

            await self.bot.send_message(
                chat_id=self.target_channel, text=message,
                parse_mode='Markdown', disable_web_page_preview=False # True Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù¾ÛŒØ´â€ŒÙ†Ù…Ø§ÛŒØ´ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø§Ú¯Ø± Ø´Ù„ÙˆØº Ù…ÛŒâ€ŒØ´ÙˆØ¯
            )
            logger.info(f"Published event: {event.title[:60]}... from {event.source_channel}")
        except Exception as e:
            logger.error(f"Failed to publish event ({event.title[:60]}...): {e}", exc_info=True)

    async def run_monitoring_loop(self):
        logger.info("Starting RSS monitoring...")
        await asyncio.sleep(10) 
        while True:
            logger.info("Checking all feeds...")
            all_new_events = []
            async with aiohttp.ClientSession() as session:
                tasks = [self.fetch_feed(session, feed_info) for feed_info in self.rss_feeds]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                for result in results:
                    if isinstance(result, list): all_new_events.extend(result)
                    elif isinstance(result, Exception): logger.error(f"Feed task error: {result}", exc_info=result)
            
            if all_new_events:
                logger.info(f"Found {len(all_new_events)} new event(s).")
                for event_to_publish in all_new_events:
                    await self.publish_event(event_to_publish)
                    await asyncio.sleep(5) 
            else:
                logger.info("No new events found.")
            
            check_interval_seconds = 600 
            logger.info(f"Next check in {check_interval_seconds // 60} minutes.")
            await asyncio.sleep(check_interval_seconds)


# ... (Ú©Ù„Ø§Ø³ Config Ùˆ ØªÙˆØ§Ø¨Ø¹ health_check, start_web_server, main Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø§Ø² Ù¾Ø§Ø³Ø® Ù‚Ø¨Ù„ÛŒ) ...
class Config:
    BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')
    TARGET_CHANNEL = os.getenv('TARGET_CHANNEL')

rss_bot_instance: Optional[RSSTelegramBot] = None 

async def health_check(request):
    global rss_bot_instance
    if rss_bot_instance:
        return web.Response(text=f"Bot is running! Processed items: {len(rss_bot_instance.processed_items)}", status=200)
    return web.Response(text="Bot not fully initialized.", status=200)

async def start_web_server():
    app = web.Application()
    app.router.add_get('/health', health_check)
    app.router.add_get('/', health_check)
    runner = web.AppRunner(app)
    await runner.setup()
    port = int(os.environ.get("PORT", "7860"))
    site = web.TCPSite(runner, '0.0.0.0', port)
    try:
        await site.start()
        logger.info(f"Web server started on port {port}")
    except OSError as e:
        logger.error(f"Failed to start web server on port {port}: {e}.")

async def main():
    global rss_bot_instance
    if not Config.BOT_TOKEN or not Config.TARGET_CHANNEL:
        logger.critical("Missing TELEGRAM_BOT_TOKEN or TARGET_CHANNEL environment variables!")
        return
    
    logger.info("Application starting...")
    rss_bot_instance = RSSTelegramBot(bot_token=Config.BOT_TOKEN, target_channel=Config.TARGET_CHANNEL)
    try:
        bot_info = await rss_bot_instance.bot.get_me()
        logger.info(f"Bot started: @{bot_info.username}")
        web_server_task = asyncio.create_task(start_web_server())
        await rss_bot_instance.run_monitoring_loop()
    except KeyboardInterrupt:
        logger.info("Bot stopped by user.")
    except Exception as e:
        logger.error(f"Critical bot error in main: {e}", exc_info=True)
    finally:
        logger.info("Bot shutting down...")
        if 'web_server_task' in locals() and web_server_task and not web_server_task.done(): #locals() Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
            web_server_task.cancel()
            try: await web_server_task
            except asyncio.CancelledError: logger.info("Web server task cancelled.")
        logger.info("Application fully stopped.")

if __name__ == "__main__":
    asyncio.run(main())
