import asyncio
import re
import logging
from datetime import datetime
import aiohttp
import feedparser
from telegram import Bot
import os
from dataclasses import dataclass
from typing import List, Optional
from bs4 import BeautifulSoup # <--- Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
from aiohttp import web

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class EventInfo:
    """Class to store event information"""
    title: str
    description: str  # This will store the raw HTML description
    link: str
    published: str
    source_channel: str
    source_channel_username: Optional[str] = None


class EventDetector:
    """Class to detect events in RSS feed content"""
    EVENT_KEYWORDS = [
        'ÙˆØ¨ÛŒÙ†Ø§Ø±', 'webinar', 'Ú©Ø§Ø±Ú¯Ø§Ù‡', 'workshop', 'Ø³Ù…ÛŒÙ†Ø§Ø±', 'seminar',
        'Ú©Ù†ÙØ±Ø§Ù†Ø³', 'conference', 'Ù‡Ù…Ø§ÛŒØ´', 'congress', 'Ù†Ø´Ø³Øª', 'meeting',
        'Ø¯ÙˆØ±Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ', 'course', 'Ú©Ù„Ø§Ø³', 'class', 'Ø§ÛŒÙˆÙ†Øª', 'event',
        'Ø¨Ø±Ú¯Ø²Ø§Ø±', 'organize', 'Ø´Ø±Ú©Øª', 'participate', 'Ø«Ø¨Øª Ù†Ø§Ù…', 'register',
        'Ø±Ø§ÛŒÚ¯Ø§Ù†', 'free', 'Ø¢Ù†Ù„Ø§ÛŒÙ†', 'online', 'Ù…Ø¬Ø§Ø²ÛŒ', 'virtual',
        'Ø¢Ù…ÙˆØ²Ø´', 'training', 'ÙØ±Ø§Ø®ÙˆØ§Ù†', 'call', 'Ú¯ÙˆØ§Ù‡ÛŒ', 'certificate',
        'Ù…Ø¯Ø±Ú©', 'certification', 'Ù„Ø§ÛŒÙˆ', 'live'
    ]

    def detect_event(self, title: str, description_html: str) -> bool:
        """Detect if content contains event information based on title and HTML description."""
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² BeautifulSoup Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† ØªÙ…ÛŒØ²ØªØ± Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ
        soup = BeautifulSoup(description_html, "html.parser")
        # Ø­Ø°Ù Ø¨Ø®Ø´ "Forwarded From" Ù‚Ø¨Ù„ Ø§Ø² Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø¨Ù‡ØªØ±
        # Ø§ÛŒÙ† Ú©Ø§Ø± Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø¨Ø®Ø´ ÙÙˆØ±ÙˆØ§Ø±Ø¯ Ø´Ø¯Ù‡ØŒ Ø¨Ù‡ Ø§Ø´ØªØ¨Ø§Ù‡ Ø¨Ø§Ø¹Ø« ØªØ´Ø®ÛŒØµ Ø±ÙˆÛŒØ¯Ø§Ø¯ Ù†Ø´ÙˆÙ†Ø¯
        # Ø§Ú¯Ø±Ú†Ù‡ Ù…Ù†Ø·Ù‚ Ø§ØµÙ„ÛŒ Ø­Ø°Ù "Forwarded From" Ø¯Ø± format_event_message Ø§Ø³Øª.
        # Ø§ÛŒÙ†Ø¬Ø§ ÛŒÚ© Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ….
        possible_forward_header = soup.find('p') # Ø§ÙˆÙ„ÛŒÙ† Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù
        if possible_forward_header and possible_forward_header.get_text(strip=True).lower().startswith("forwarded from"):
            possible_forward_header.extract()

        text_only_description = soup.get_text(separator=' ', strip=True) # separator=' ' Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ú†Ø³Ø¨ÛŒØ¯Ù† Ú©Ù„Ù…Ø§Øª
        full_text = f"{title} {text_only_description}"
        text_lower = full_text.lower()

        matches = sum(1 for keyword in self.EVENT_KEYWORDS if keyword in text_lower)
        has_specific_pattern = any([
            'Ø«Ø¨Øª Ù†Ø§Ù…' in text_lower, 'Ø´Ø±Ú©Øª Ø¯Ø±' in text_lower, 'Ø¨Ø±Ú¯Ø²Ø§Ø± Ù…ÛŒ' in text_lower,
            'register' in text_lower, 'join' in text_lower, 'ÙˆØ¨ÛŒÙ†Ø§Ø±' in text_lower,
            'Ú©Ø§Ø±Ú¯Ø§Ù‡' in text_lower, 'Ø¯ÙˆØ±Ù‡ Ø¢Ù†Ù„Ø§ÛŒÙ†' in text_lower
        ])
        return matches >= 1 or has_specific_pattern # Ø¢Ø³ØªØ§Ù†Ù‡ Ø±Ø§ Ø¨Ù‡ Û± Ú©Ø§Ù‡Ø´ Ø¯Ø§Ø¯Ù…ØŒ Ú†ÙˆÙ† Ø¨Ø§ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¨Ù‡ØªØ±ØŒ ÛŒÚ© Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ Ù‡Ù… Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ú©Ø§ÙÛŒ Ø¨Ø§Ø´Ø¯.


class RSSTelegramBot:
    """RSS-based Telegram Event Bot"""

    def __init__(self, bot_token: str, target_channel: str):
        self.bot_token = bot_token
        self.target_channel = target_channel
        self.bot = Bot(token=bot_token)
        self.detector = EventDetector()
        self.processed_items = set()

        self.rss_feeds = [
            {'name': 'WinCell Co', 'url': 'https://rsshub.app/telegram/channel/wincellco', 'channel': 'wincellco'},
            {'name': 'Rayazistazma', 'url': 'https://rsshub.app/telegram/channel/Rayazistazma', 'channel': 'Rayazistazma'},
            {'name': 'SBU Bio Society', 'url': 'https://rsshub.app/telegram/channel/SBUBIOSOCIETY', 'channel': 'SBUBIOSOCIETY'},
            {'name': 'Test BioPy Channel', 'url': 'https://rsshub.app/telegram/channel/testbiopy', 'channel': 'testbiopy'}
        ]

    async def fetch_feed(self, session: aiohttp.ClientSession, feed_info: dict) -> List[EventInfo]:
        events = []
        feed_url, feed_name = feed_info['url'], feed_info['name']
        logger.info(f"Fetching feed: {feed_name} from {feed_url}")
        try:
            async with session.get(feed_url, timeout=45) as response: # Ø§ÙØ²Ø§ÛŒØ´ ØªØ§ÛŒÙ…â€ŒØ§ÙˆØª
                if response.status == 200:
                    content = await response.text()
                    feed = feedparser.parse(content)
                    logger.info(f"Fetched {feed_name}. Entries: {len(feed.entries)}")
                    for entry in feed.entries[:10]: # ÛŒØ§ Ù‡Ø± ØªØ¹Ø¯Ø§Ø¯ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ø§ÙˆÙ„ÛŒÙ‡ Ù…Ù†Ø§Ø³Ø¨ Ù…ÛŒâ€ŒØ¯Ø§Ù†ÛŒØ¯ØŒ Ù…Ø«Ù„Ø§ Ûµ
                        entry_id = f"{feed_info.get('channel', feed_name)}_{entry.get('id', entry.get('link', ''))}"
                        if entry_id not in self.processed_items:
                            raw_title = entry.get('title', '').strip()
                            raw_description_html = entry.get('description', entry.get('summary', ''))
                            if self.detector.detect_event(raw_title, raw_description_html):
                                event = EventInfo(
                                    title=raw_title, description=raw_description_html,
                                    link=entry.get('link', ''), published=entry.get('published', ''),
                                    source_channel=feed_name, source_channel_username=feed_info.get('channel')
                                )
                                events.append(event)
                                self.processed_items.add(entry_id)
                        if len(self.processed_items) > 1500: # Ú©Ù…ÛŒ Ø§ÙØ²Ø§ÛŒØ´ Ø¸Ø±ÙÛŒØª Ø­Ø§ÙØ¸Ù‡
                            self.processed_items = set(list(self.processed_items)[500:])
                            logger.info(f"Cleaned up processed_items. New size: {len(self.processed_items)}")
                else:
                    logger.error(f"Error fetching {feed_name}: Status {response.status} - {await response.text(encoding='utf-8', errors='ignore')}")
        except Exception as e:
            logger.error(f"Exception in fetch_feed for {feed_name} ({feed_url}): {e}", exc_info=True)
        return events

    def _clean_html_and_extract_text(self, html_content: str) -> str:
        """
        Cleans HTML content using BeautifulSoup to produce well-formatted plain text.
        - Removes "Forwarded From" sections.
        - Converts <br> to newlines.
        - Handles <p> and list elements for better paragraph separation.
        - Strips other HTML tags.
        """
        if not html_content:
            return ""

        soup = BeautifulSoup(html_content, "html.parser") # ÛŒØ§ 'lxml' Ø§Ú¯Ø± Ù†ØµØ¨ Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒØ¯

        # 1. Ø­Ø°Ù Ø¨Ø®Ø´ "Forwarded From" (Ù…Ø¹Ù…ÙˆÙ„Ø§ Ø§ÙˆÙ„ÛŒÙ† Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒ RSSHub Ø§Ø³Øª)
        first_p = soup.find('p')
        if first_p and first_p.get_text(strip=True).lower().startswith("forwarded from"):
            logger.debug(f"Removing 'Forwarded From' paragraph: {first_p.prettify()}")
            first_p.decompose() # Ø­Ø°Ù Ú©Ø§Ù…Ù„ ØªÚ¯ p Ùˆ Ù…Ø­ØªÙˆÛŒØ§ØªØ´

        # 2. ØªØ¨Ø¯ÛŒÙ„ ØªÚ¯â€ŒÙ‡Ø§ÛŒ <br> Ø¨Ù‡ Ø®Ø· Ø¬Ø¯ÛŒØ¯ Ù‚Ø¨Ù„ Ø§Ø² Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ†
        for br in soup.find_all("br"):
            br.replace_with("\n")

        # 3. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø¨Ø§ Ø­ÙØ¸ Ø³Ø§Ø®ØªØ§Ø± Ø¨Ù‡ØªØ± Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ Ùˆ Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§
        # get_text Ø¨Ø§ separator='\n' Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¨Ù„Ø§Ú© ØªÚ¯ ÛŒÚ© Ø®Ø· Ø¬Ø¯ÛŒØ¯ Ù…ÛŒâ€ŒÚ¯Ø°Ø§Ø±Ø¯
        # Ùˆ strip=True ÙØ¶Ø§Ù‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ Ø§Ø¶Ø§ÙÛŒ Ø±Ø§ Ø­Ø°Ù Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        text_blocks = []
        for element in soup.find_all(['p', 'div', 'li', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
            # Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¨Ù„Ø§Ú©ØŒ Ù…ØªÙ† Ø±Ø§ Ú¯Ø±ÙØªÙ‡ Ùˆ strip Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…. Ø®Ø·ÙˆØ· Ø®Ø§Ù„ÛŒ Ø±Ø§ Ø¨Ø¹Ø¯Ø§ Ø­Ø°Ù Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
            block_text = element.get_text(separator=' ', strip=True) # separator=' ' Ø¨Ø±Ø§ÛŒ Ú©Ù„Ù…Ø§Øª Ø¯Ø§Ø®Ù„ ÛŒÚ© Ø¨Ù„Ø§Ú©
            if block_text:
                text_blocks.append(block_text)
        
        if not text_blocks: # Ø§Ú¯Ø± Ø³Ø§Ø®ØªØ§Ø± p, div Ùˆ ØºÛŒØ±Ù‡ Ù†Ø¨ÙˆØ¯ØŒ Ú©Ù„ Ù…ØªÙ† Ø±Ø§ Ø¨Ú¯ÛŒØ±
             text_blocks.append(soup.get_text(separator='\n', strip=True))


        # Ø¨Ù‡ Ù‡Ù… Ú†Ø³Ø¨Ø§Ù†Ø¯Ù† Ø¨Ù„Ø§Ú©â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø¨Ø§ ÛŒÚ© Ø®Ø· Ø¬Ø¯ÛŒØ¯ Ø¨ÛŒÙ† Ø¢Ù†â€ŒÙ‡Ø§
        full_text = "\n".join(text_blocks)

        # Ø­Ø°Ù Ø®Ø·ÙˆØ·ÛŒ Ú©Ù‡ ÙÙ‚Ø· Ø´Ø§Ù…Ù„ ÙØ¶Ø§Ù‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ Ù‡Ø³ØªÙ†Ø¯ Ùˆ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø´Ú©Ø³ØªÚ¯ÛŒ Ø®Ø·ÙˆØ·
        lines = [line.strip() for line in full_text.splitlines()]
        cleaned_text = "\n".join(filter(None, lines)) # Ø­Ø°Ù Ø®Ø·ÙˆØ· Ú©Ø§Ù…Ù„Ø§ Ø®Ø§Ù„ÛŒ

        return cleaned_text

    def format_event_message(self, event: EventInfo) -> str:
        RLM = "\u200F"

        display_title = event.title.strip()
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¹Ù†ÙˆØ§Ù† Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ (Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø§Ø² Ú©Ø¯ Ù‚Ø¨Ù„ÛŒ Ø­ÙØ¸ Ø´Ø¯Ù‡ Ùˆ Ø®ÙˆØ¨ Ø§Ø³Øª)
        normalized_title_for_comparison = re.sub(r"^[ğŸ”ğŸ–¼âšœï¸ğŸ“ğŸ“¢âœ”ï¸âœ…ğŸ”†ğŸ—“ï¸ğŸ“ğŸ’³#Ùªâ™¦ï¸ğŸ”¹ğŸ”¸ğŸŸ¢â™¦ï¸â–ªï¸â–«ï¸â–ªï¸â€¢â—ğŸ”˜ğŸ‘â€ğŸ—¨\s]+(?=[^\s])", "", display_title, flags=re.IGNORECASE).strip().lower()
        normalized_title_for_comparison = re.sub(r"[\s.:]*$", "", normalized_title_for_comparison)

        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…ØªØ¯ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ ØªÙˆØ¶ÛŒØ­Ø§Øª Ø¨Ø§ BeautifulSoup
        description_cleaned_text = self._clean_html_and_extract_text(event.description)
        
        description_to_display = description_cleaned_text
        if description_cleaned_text and normalized_title_for_comparison:
            first_desc_line = description_cleaned_text.split('\n', 1)[0].strip()
            normalized_first_desc_line = re.sub(r"^[ğŸ”ğŸ–¼âšœï¸ğŸ“ğŸ“¢âœ”ï¸âœ…ğŸ”†ğŸ—“ï¸ğŸ“ğŸ’³#Ùªâ™¦ï¸ğŸ”¹ğŸ”¸ğŸŸ¢â™¦ï¸â–ªï¸â–«ï¸â–ªï¸â€¢â—ğŸ”˜ğŸ‘â€ğŸ—¨\s]+(?=[^\s])", "", first_desc_line, flags=re.IGNORECASE).strip().lower()
            normalized_first_desc_line = re.sub(r"[\s.:]*$", "", normalized_first_desc_line)

            # Ø§Ú¯Ø± Ø¹Ù†ÙˆØ§Ù† Ùˆ Ø®Ø· Ø§ÙˆÙ„ ØªÙˆØ¶ÛŒØ­Ø§Øª ÛŒÚ©ÛŒ Ø¨ÙˆØ¯Ù†Ø¯ ÛŒØ§ Ø¹Ù†ÙˆØ§Ù† Ù¾ÛŒØ´ÙˆÙ†Ø¯ÛŒ Ø§Ø² Ø¢Ù† Ø¨ÙˆØ¯
            if normalized_title_for_comparison == normalized_first_desc_line or \
               (len(normalized_title_for_comparison) > 15 and normalized_first_desc_line.startswith(normalized_title_for_comparison)): # Ø´Ø±Ø· Ø·ÙˆÙ„ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´ÙˆÙ†Ø¯Ù‡Ø§ÛŒ Ù…Ø¹Ù†Ø§Ø¯Ø§Ø±
                
                if '\n' in description_cleaned_text:
                    description_to_display = description_cleaned_text.split('\n', 1)[1].strip()
                else:
                    description_to_display = ""
                
                temp_lines = [line.strip() for line in description_to_display.splitlines()]
                description_to_display = "\n".join(filter(None, temp_lines))

        DESCRIPTION_MAX_LEN = 1500 # Ø§ÙØ²Ø§ÛŒØ´ Ø¨ÛŒØ´ØªØ± Ù…Ø­Ø¯ÙˆØ¯ÛŒØª
        if len(description_to_display) > DESCRIPTION_MAX_LEN:
            cut_off_point = description_to_display.rfind('.', 0, DESCRIPTION_MAX_LEN)
            if cut_off_point != -1 and cut_off_point > DESCRIPTION_MAX_LEN - 300:
                 description_to_display = description_to_display[:cut_off_point+1] + " (...)"
            else:
                 description_to_display = description_to_display[:DESCRIPTION_MAX_LEN] + "..."
        
        if not description_to_display.strip():
            description_to_display = ""

        message_parts = []
        if display_title:
             message_parts.append(f"{RLM}ğŸ“ **{display_title}**")

        if description_to_display:
            message_parts.append(f"\n\n{RLM}{description_to_display}")

        meta_info_parts = []
        if event.link:
            meta_info_parts.append(f"{RLM}ğŸ”— [Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ø§Ù…Ù„ Ø±ÙˆÛŒØ¯Ø§Ø¯]({event.link})") # Ù…ØªÙ† Ù„ÛŒÙ†Ú© Ø±Ø§ Ú©Ù…ÛŒ ØªØºÛŒÛŒØ± Ø¯Ø§Ø¯Ù…
        
        if event.source_channel_username:
            meta_info_parts.append(f"{RLM}ğŸ“¢ **Ù…Ù†Ø¨Ø¹:** [{event.source_channel}](https://t.me/{event.source_channel_username})")
        else:
            meta_info_parts.append(f"{RLM}ğŸ“¢ **Ù…Ù†Ø¨Ø¹:** {event.source_channel}")

        if event.published:
            formatted_date = ""
            try:
                date_obj = datetime.strptime(event.published, "%a, %d %b %Y %H:%M:%S %Z")
                formatted_date = date_obj.strftime("%d %b %Y - %H:%M %Z")
            except ValueError:
                try:
                    main_date_part = event.published.split(',')[1].strip() if ',' in event.published else event.published
                    formatted_date = main_date_part.rsplit(':',1)[0] + " GMT"
                except: formatted_date = event.published
            if formatted_date:
                 meta_info_parts.append(f"{RLM}ğŸ“… **Ø§Ù†ØªØ´Ø§Ø±:** {formatted_date}")

        if meta_info_parts:
            separator = "\n\n" if (display_title and description_to_display) or (display_title and not description_to_display and len(message_parts) > 0) else "\n"
            if not description_to_display and display_title: separator = "\n\n" # Ø§Ú¯Ø± ÙÙ‚Ø· Ø¹Ù†ÙˆØ§Ù† Ø¨ÙˆØ¯ØŒ Ø¯Ùˆ Ø®Ø· ÙØ§ØµÙ„Ù‡ ØªØ§ Ù…ØªØ§

            message_parts.append(separator + "\n".join(meta_info_parts))

        final_message = "\n".join(filter(None, message_parts)).strip()
        
        TELEGRAM_MSG_MAX_LEN = 4096
        if len(final_message) > TELEGRAM_MSG_MAX_LEN:
            logger.warning(f"Message for '{display_title}' too long ({len(final_message)} chars), truncating.")
            # Ú©ÙˆØªØ§Ù‡ Ú©Ø±Ø¯Ù† Ù‡ÙˆØ´Ù…Ù†Ø¯Ø§Ù†Ù‡â€ŒØªØ± Ù¾ÛŒØ§Ù… Ú©Ù„ÛŒ Ø§Ú¯Ø± Ù„Ø§Ø²Ù… Ø¨Ø§Ø´Ø¯
            # Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø¯Ú¯ÛŒØŒ ÙØ¹Ù„Ø§ Ø§Ø² Ø§Ù†ØªÙ‡Ø§ÛŒ Ù¾ÛŒØ§Ù… Ú©Ù„ÛŒ Ù…ÛŒâ€ŒØ¨Ø±ÛŒÙ…
            final_message = final_message[:TELE_GRAM_MSG_MAX_LEN - 20] + "\n(... Ø§Ø¯Ø§Ù…Ù‡ Ø¯Ø§Ø±Ø¯ ...)"
            
        return final_message

    async def publish_event(self, event: EventInfo):
        try:
            message = self.format_event_message(event)
            if not message or not event.title:
                logger.info(f"Skipping empty or title-less message for an event from {event.source_channel}.")
                return

            await self.bot.send_message(
                chat_id=self.target_channel, text=message,
                parse_mode='Markdown', disable_web_page_preview=False
            )
            logger.info(f"Published event: {event.title[:60]}... from {event.source_channel}")
        except Exception as e:
            logger.error(f"Failed to publish event ({event.title[:60]}...): {e}", exc_info=True)

    async def run_monitoring_loop(self):
        logger.info("Starting RSS monitoring...")
        await asyncio.sleep(10) # Ø§ÙØ²Ø§ÛŒØ´ Ø²Ù…Ø§Ù† Ø§Ù†ØªØ¸Ø§Ø± Ø§ÙˆÙ„ÛŒÙ‡
        while True:
            logger.info("Checking all feeds...")
            all_new_events = []
            async with aiohttp.ClientSession() as session:
                tasks = [self.fetch_feed(session, feed_info) for feed_info in self.rss_feeds]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                for result in results:
                    if isinstance(result, list): all_new_events.extend(result)
                    elif isinstance(result, Exception): logger.error(f"Feed task error: {result}", exc_info=result)
            
            if all_new_events:
                logger.info(f"Found {len(all_new_events)} new event(s).")
                # Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ§Ø±ÛŒØ® Ø§Ù†ØªØ´Ø§Ø±Ø´Ø§Ù† Ù…Ø±ØªØ¨ Ú©Ù†ÛŒØ¯ (Ø§Ú¯Ø± Ù…Ø¹ØªØ¨Ø± Ø¨Ø§Ø´Ø¯)
                # all_new_events.sort(key=lambda ev: ev.published_parsed_time_object_if_available, reverse=True)
                for event_to_publish in all_new_events:
                    await self.publish_event(event_to_publish)
                    await asyncio.sleep(5) # Ø§ÙØ²Ø§ÛŒØ´ ÙØ§ØµÙ„Ù‡ Ø¨ÛŒÙ† Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§
            else:
                logger.info("No new events found.")
            
            check_interval_seconds = 600
            logger.info(f"Next check in {check_interval_seconds // 60} minutes.")
            await asyncio.sleep(check_interval_seconds)

class Config:
    BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')
    TARGET_CHANNEL = os.getenv('TARGET_CHANNEL')

rss_bot_instance: Optional[RSSTelegramBot] = None

async def health_check(request):
    global rss_bot_instance
    if rss_bot_instance:
        return web.Response(text=f"Bot is running! Processed items: {len(rss_bot_instance.processed_items)}", status=200)
    return web.Response(text="Bot not fully initialized.", status=200)

async def start_web_server():
    app = web.Application()
    app.router.add_get('/health', health_check)
    app.router.add_get('/', health_check)
    runner = web.AppRunner(app)
    await runner.setup()
    port = int(os.environ.get("PORT", "7860"))
    site = web.TCPSite(runner, '0.0.0.0', port)
    try:
        await site.start()
        logger.info(f"Web server started on port {port}")
    except OSError as e:
        logger.error(f"Failed to start web server on port {port}: {e}.")

async def main():
    global rss_bot_instance
    if not Config.BOT_TOKEN or not Config.TARGET_CHANNEL:
        logger.critical("Missing TELEGRAM_BOT_TOKEN or TARGET_CHANNEL environment variables!")
        return
    
    logger.info("Application starting...")
    rss_bot_instance = RSSTelegramBot(bot_token=Config.BOT_TOKEN, target_channel=Config.TARGET_CHANNEL)
    try:
        bot_info = await rss_bot_instance.bot.get_me()
        logger.info(f"Bot started: @{bot_info.username}")
        web_server_task = asyncio.create_task(start_web_server())
        await rss_bot_instance.run_monitoring_loop()
    except KeyboardInterrupt:
        logger.info("Bot stopped by user.")
    except Exception as e:
        logger.error(f"Critical bot error in main: {e}", exc_info=True)
    finally:
        logger.info("Bot shutting down...")
        if 'web_server_task' in locals() and web_server_task and not web_server_task.done():
            web_server_task.cancel()
            try: await web_server_task
            except asyncio.CancelledError: logger.info("Web server task cancelled.")
        logger.info("Application fully stopped.")

if __name__ == "__main__":
    asyncio.run(main())
