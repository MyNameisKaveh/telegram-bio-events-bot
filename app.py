import asyncio
import re
import logging
from datetime import datetime
import html # Ø¨Ø±Ø§ÛŒ unescape Ø§ÙˆÙ„ÛŒÙ‡ Ø§Ú¯Ø± Ù„Ø§Ø²Ù… Ø¨Ø§Ø´Ø¯
import aiohttp
import feedparser
from telegram import Bot
from telegram.constants import ParseMode
from telegram.helpers import escape_markdown # Ø¨Ø±Ø§ÛŒ MarkdownV2 escaping
import os
from dataclasses import dataclass
from typing import List, Optional
from bs4 import BeautifulSoup, NavigableString, Tag
from aiohttp import web

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class EventInfo:
    title: str # Ù‡Ù…Ú†Ù†Ø§Ù† Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø±ÙˆÛŒØ¯Ø§Ø¯ Ùˆ Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ù„Ø§Ø²Ù… Ø§Ø³Øª
    description: str  # HTML Ø®Ø§Ù… Ø§Ø² ÙÛŒØ¯
    link: str
    published: str
    source_channel: str
    source_channel_username: Optional[str] = None

class EventDetector:
    EVENT_KEYWORDS = [
        'ÙˆØ¨ÛŒÙ†Ø§Ø±', 'webinar', 'Ú©Ø§Ø±Ú¯Ø§Ù‡', 'workshop', 'Ø³Ù…ÛŒÙ†Ø§Ø±', 'seminar', 'Ú©Ù†ÙØ±Ø§Ù†Ø³', 'conference', 
        'Ù‡Ù…Ø§ÛŒØ´', 'congress', 'Ù†Ø´Ø³Øª', 'meeting', 'Ø¯ÙˆØ±Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ', 'course', 'Ú©Ù„Ø§Ø³', 'class', 
        'Ø§ÛŒÙˆÙ†Øª', 'event', 'Ø¨Ø±Ú¯Ø²Ø§Ø±', 'organize', 'Ø´Ø±Ú©Øª', 'participate', 'Ø«Ø¨Øª Ù†Ø§Ù…', 'register',
        'Ø±Ø§ÛŒÚ¯Ø§Ù†', 'free', 'Ø¢Ù†Ù„Ø§ÛŒÙ†', 'online', 'Ù…Ø¬Ø§Ø²ÛŒ', 'virtual', 'Ø¢Ù…ÙˆØ²Ø´', 'training', 
        'ÙØ±Ø§Ø®ÙˆØ§Ù†', 'call', 'Ú¯ÙˆØ§Ù‡ÛŒ', 'certificate', 'Ù…Ø¯Ø±Ú©', 'certification', 'Ù„Ø§ÛŒÙˆ', 'live'
    ]

    def detect_event(self, title: str, description_html: str) -> bool:
        # (Ù…Ù†Ø·Ù‚ ØªØ´Ø®ÛŒØµ Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø§Ø² Ù¾Ø§Ø³Ø® Ù‚Ø¨Ù„ÛŒØŒ Ú©Ù‡ Ø¨Ù‡ Ù†Ø¸Ø± Ø´Ù…Ø§ Ø®ÙˆØ¨ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ø±Ø¯)
        soup = BeautifulSoup(description_html, "html.parser")
        first_p_tag = soup.find('p')
        if first_p_tag and first_p_tag.get_text(strip=True).lower().startswith("forwarded from"):
            first_p_tag.decompose()
        
        text_only_description = soup.get_text(separator=' ', strip=True)
        
        title_lower = title.lower()
        strong_title_keywords = ['ÙˆØ¨ÛŒÙ†Ø§Ø±', 'Ú©Ø§Ø±Ú¯Ø§Ù‡', 'Ø³Ù…ÛŒÙ†Ø§Ø±', 'Ø¯ÙˆØ±Ù‡', 'Ú©Ù†ÙØ±Ø§Ù†Ø³', 'Ù‡Ù…Ø§ÛŒØ´', 'Ù†Ø´Ø³Øª Ø¢Ù…ÙˆØ²Ø´ÛŒ', 'Ú©Ù„Ø§Ø³ Ø¢Ù†Ù„Ø§ÛŒÙ†']
        if any(keyword in title_lower for keyword in strong_title_keywords):
            return True

        full_text_desc_lower = text_only_description.lower()
        combined_text_lower = f"{title_lower} {full_text_desc_lower}"
        matches = sum(1 for keyword in self.EVENT_KEYWORDS if keyword in combined_text_lower)
        
        has_specific_pattern = any([
            'Ø«Ø¨Øª Ù†Ø§Ù…' in combined_text_lower, 'Ø´Ø±Ú©Øª Ø¯Ø±' in combined_text_lower,
            'Ù„ÛŒÙ†Ú© Ø«Ø¨Øª Ù†Ø§Ù…' in combined_text_lower, 'Ø¬Ù‡Øª Ø«Ø¨Øª Ù†Ø§Ù…' in combined_text_lower,
            'Ù‡Ø²ÛŒÙ†Ù‡ Ø¯ÙˆØ±Ù‡' in full_text_desc_lower, 'Ø³Ø±ÙØµÙ„ Ù‡Ø§ÛŒ Ø¯ÙˆØ±Ù‡' in full_text_desc_lower,
            'Ù…Ø¯Ø±Ø³ Ø¯ÙˆØ±Ù‡' in full_text_desc_lower, 'Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒØ´ØªØ± Ùˆ Ø«Ø¨Øª Ù†Ø§Ù…' in full_text_desc_lower,
            'register for' in combined_text_lower, 'join this' in combined_text_lower
        ])
        
        title_has_keyword = any(keyword in title_lower for keyword in ['Ø¢Ù…ÙˆØ²Ø´', 'ÙØ±Ø§Ø®ÙˆØ§Ù†', 'Ù„Ø§ÛŒÙˆ'])
        return title_has_keyword or has_specific_pattern or matches >= 2


class RSSTelegramBot:
    def __init__(self, bot_token: str, target_channel: str):
        self.bot_token = bot_token
        self.target_channel = target_channel
        self.bot = Bot(token=bot_token)
        self.detector = EventDetector()
        self.processed_items = set()
        self.rss_feeds = [
            {'name': 'WinCell Co', 'url': 'https://rsshub.app/telegram/channel/wincellco', 'channel': 'wincellco'},
            {'name': 'Rayazistazma', 'url': 'https://rsshub.app/telegram/channel/Rayazistazma', 'channel': 'Rayazistazma'},
            {'name': 'SBU Bio Society', 'url': 'https://rsshub.app/telegram/channel/SBUBIOSOCIETY', 'channel': 'SBUBIOSOCIETY'},
            {'name': 'Test BioPy Channel', 'url': 'https://rsshub.app/telegram/channel/testbiopy', 'channel': 'testbiopy'}
        ]
        self.RLM = "\u200F"

    async def fetch_feed(self, session: aiohttp.ClientSession, feed_info: dict) -> List[EventInfo]:
        # (Ø§ÛŒÙ† Ù…ØªØ¯ Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø§Ø² Ù¾Ø§Ø³Ø® Ù‚Ø¨Ù„ÛŒ)
        events = []
        feed_url, feed_name = feed_info['url'], feed_info['name']
        logger.info(f"Fetching feed: {feed_name} from {feed_url}")
        try:
            headers = {'Cache-Control': 'no-cache', 'Pragma': 'no-cache'}
            async with session.get(feed_url, timeout=45, headers=headers) as response:
                if response.status == 200:
                    content = await response.text()
                    feed = feedparser.parse(content)
                    logger.info(f"Fetched {feed_name}. Entries: {len(feed.entries)}. LastBuildDate: {feed.feed.get('updated', feed.feed.get('published', 'N/A'))}")
                    for entry in feed.entries[:7]: 
                        entry_id = f"{feed_info.get('channel', feed_name)}_{entry.get('id', entry.get('link', ''))}"
                        if entry_id not in self.processed_items:
                            raw_title = entry.get('title', '').strip()
                            raw_description_html = entry.get('description', entry.get('summary', ''))
                            if self.detector.detect_event(raw_title, raw_description_html):
                                event = EventInfo(
                                    title=raw_title, description=raw_description_html,
                                    link=entry.get('link', ''), published=entry.get('published', ''),
                                    source_channel=feed_name, source_channel_username=feed_info.get('channel')
                                )
                                events.append(event)
                                self.processed_items.add(entry_id)
                        if len(self.processed_items) > 1500:
                            self.processed_items = set(list(self.processed_items)[500:])
                else:
                    logger.error(f"Error fetching {feed_name}: Status {response.status} - {await response.text(encoding='utf-8', errors='ignore')}")
        except Exception as e:
            logger.error(f"Exception in fetch_feed for {feed_name} ({feed_url}): {e}", exc_info=True)
        return events

    def _escape_md_v2(self, text: Optional[str]) -> str:
        if text is None: return ""
        return escape_markdown(str(text), version=2)

    def _convert_node_to_markdown_v2_recursive(self, element, list_level=0, inside_pre=False) -> str:
        if isinstance(element, NavigableString):
            # Ù…ØªÙ†â€ŒÙ‡Ø§ Ø¨Ø§ÛŒØ¯ escape Ø´ÙˆÙ†Ø¯ Ù…Ú¯Ø± Ø§ÛŒÙ†Ú©Ù‡ Ø¯Ø§Ø®Ù„ <pre> Ø¨Ø§Ø´Ù†Ø¯
            if inside_pre:
                return str(element) # Ù…Ø­ØªÙˆØ§ÛŒ Ø¯Ø§Ø®Ù„ <pre> Ù†Ø¨Ø§ÛŒØ¯ escape Ø´ÙˆØ¯
            return self._escape_md_v2(str(element))
        
        tag_name = element.name
        
        # ØªØ¹ÛŒÛŒÙ† Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ ÙØ±Ø²Ù†Ø¯Ø§Ù† Ø§ÛŒÙ† ØªÚ¯ Ù‡Ù… Ø¨Ø§ÛŒØ¯ Ø¯Ø± Ø­Ø§Ù„Øª inside_pre Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´ÙˆÙ†Ø¯
        children_inside_pre = inside_pre or (tag_name == 'pre')

        children_md_parts = [self._convert_node_to_markdown_v2_recursive(child, list_level + (1 if tag_name in ['ul', 'ol'] else 0), children_inside_pre) for child in element.children]
        children_md = "".join(children_md_parts)

        if tag_name in ['b', 'strong']: return f"*{children_md}*"
        if tag_name in ['i', 'em']: return f"_{children_md}_"
        if tag_name == 'u': return f"__{children_md}__"
        if tag_name in ['s', 'strike', 'del']: return f"~{children_md}~"
        
        if tag_name == 'code':
            if element.parent.name == 'pre': # Ø§Ú¯Ø± code Ø¯Ø§Ø®Ù„ pre Ø¨ÙˆØ¯
                return children_md # Ø®ÙˆØ¯ pre ÙØ±Ù…Øª ``` Ø±Ø§ Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯ØŒ Ø§ÛŒÙ†Ø¬Ø§ ÙÙ‚Ø· Ù…Ø­ØªÙˆØ§ÛŒ Ø®Ø§Ù…
            # Ø¨Ø±Ø§ÛŒ Ú©Ø¯ inlineØŒ Ù…Ø­ØªÙˆØ§ Ù†Ø¨Ø§ÛŒØ¯ Ø´Ø§Ù…Ù„ ` Ø¨Ø§Ø´Ø¯ ÛŒØ§ Ø¨Ø§ÛŒØ¯ Ù…Ø¯ÛŒØ±ÛŒØª Ø´ÙˆØ¯
            # ØªÙ„Ú¯Ø±Ø§Ù… Ø§Ø² Ø¯Ùˆ Ø¨Ú©â€ŒØªÛŒÚ© Ø¨Ø±Ø§ÛŒ escape Ú©Ø±Ø¯Ù† Ø¨Ú©â€ŒØªÛŒÚ© Ø¯Ø§Ø®Ù„ Ú©Ø¯ inline Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
            # Ù¾Ø³ Ø§Ú¯Ø± Ø¨Ú©â€ŒØªÛŒÚ© Ø¯Ø§Ø´ØªØŒ Ù†Ù…Ø§ÛŒØ´ Ø¢Ù† Ø¯Ø± Ú©Ø¯ inline Ø¯Ø´ÙˆØ§Ø± Ø§Ø³Øª. ÙØ¹Ù„Ø§ Ø³Ø§Ø¯Ù‡ Ù†Ú¯Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ø±ÛŒÙ….
            return f"`{children_md}`" 

        if tag_name == 'pre':
            code_child = element.find('code', class_=lambda x: x and isinstance(x, list) and x[0].startswith('language-'))
            # children_md Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ù…Ø­ØªÙˆØ§ÛŒ Ø®Ø§Ù… (escape Ù†Ø´Ø¯Ù‡) Ø¯Ø§Ø®Ù„ <pre> Ø§Ø³Øª
            if code_child:
                lang = self._escape_md_v2(code_child['class'][0].split('-',1)[1])
                # Ù…Ø­ØªÙˆØ§ÛŒ Ú©Ø¯ Ø±Ø§ Ø§Ø² Ø®ÙˆØ¯ code_child Ø¨Ú¯ÛŒØ±ÛŒÙ… Ú†ÙˆÙ† children_md Ø´Ø§Ù…Ù„ ØªÚ¯ <code> Ù‡Ù… Ù‡Ø³Øª
                # Ùˆ Ø¨Ø§ÛŒØ¯ Ù…ØªÙ† Ø®Ø§Ù… Ø¯Ø§Ø®Ù„ <code> Ø±Ø§ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒÙ…
                pre_content = "".join(self._convert_node_to_markdown_v2_recursive(c, 0, True) for c in code_child.children)
                return f"```{lang}\n{pre_content.strip()}\n```"
            return f"```\n{children_md.strip()}\n```"

        if tag_name == 'a':
            href = element.get('href', '')
            if href and href.strip().lower().startswith(('http', 'tg://')):
                safe_href = href.strip().replace('(', '%28').replace(')', '%29')
                # children_md (Ù…ØªÙ† Ù„ÛŒÙ†Ú©) Ø§Ø² Ù‚Ø¨Ù„ escape Ø´Ø¯Ù‡ Ø§Ø³Øª
                return f"[{children_md}]({safe_href})"
            return children_md 

        if tag_name == 'br': return '\n'
        
        # ØªÚ¯â€ŒÙ‡Ø§ÛŒ Ø¨Ù„Ø§Ú© Ú©Ù‡ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø®Ø· Ø¬Ø¯ÛŒØ¯ Ù‚Ø¨Ù„ Ùˆ Ø¨Ø¹Ø¯ Ø¯Ø§Ø±Ù†Ø¯
        if tag_name in ['p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'blockquote', 'hr', 'table', 'figure']:
            # Ø§Ú¯Ø± children_md Ø¨Ø§ \n ØªÙ…Ø§Ù… Ù†Ø´Ø¯Ù‡ØŒ Ø¯Ùˆ ØªØ§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†. Ø§Ú¯Ø± Ø¨Ø§ ÛŒÚ© \n ØªÙ…Ø§Ù… Ø´Ø¯Ù‡ØŒ ÛŒÚ©ÛŒ Ø¯ÛŒÚ¯Ø± Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†.
            processed_content = children_md.strip('\n') # \n Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ Ø§Ø² Ø§Ù†ØªÙ‡Ø§ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ Ø­Ø°Ù Ø´ÙˆÙ†Ø¯
            if processed_content: # ÙÙ‚Ø· Ø§Ú¯Ø± Ù…Ø­ØªÙˆØ§ Ø¯Ø§Ø´Øª
                return f"{processed_content}\n\n"
            return "\n\n" # Ø§Ú¯Ø± Ø¨Ù„Ø§Ú© Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯ Ù‡Ù… ÛŒÚ© ÙØ§ØµÙ„Ù‡ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†Ø¯ (ÛŒØ§ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§ÛŒÙ† Ø±Ø§ Ø­Ø°Ù Ú©Ø±Ø¯)
        
        if tag_name == 'ul': return children_md 
        if tag_name == 'ol': return children_md # Ø´Ù…Ø§Ø±Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø§ØªÙˆÙ…Ø§ØªÛŒÚ© Markdown Ø¨Ø±Ø§ÛŒ ol Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø§Ø³ØªØŒ ÙØ¹Ù„Ø§ Ø´Ø¨ÛŒÙ‡ ul

        if tag_name == 'li':
            prefix = f"{self.RLM}â€¢ " 
            # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ø¨Ø§ \n ØªÙ…Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ ÙØ¶Ø§Ù‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ Ø§Ø¶Ø§ÙÛŒ Ù†Ø¯Ø§Ø±Ø¯
            return f"{prefix}{children_md.strip()}\n"

        return children_md # Ø¨Ø±Ø§ÛŒ ØªÚ¯â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡ØŒ ÙÙ‚Ø· Ù…Ø­ØªÙˆØ§ÛŒ ÙØ±Ø²Ù†Ø¯Ø§Ù†

    def _prepare_description_for_markdown_v2(self, html_content: str) -> str:
        if not html_content: return ""
        soup = BeautifulSoup(html_content, "html.parser")

        first_p = soup.find('p', recursive=False)
        if first_p and first_p.get_text(strip=True).lower().startswith("forwarded from"):
            first_p.decompose()
        
        markdown_text = self._convert_node_to_markdown_v2_recursive(soup.body if soup.body else soup)
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø®Ø·ÙˆØ· Ø¬Ø¯ÛŒØ¯
        markdown_text = markdown_text.replace('\r\n', '\n').replace('\r', '\n')
        markdown_text = re.sub(r'\n{3,}', '\n\n', markdown_text) # Ø¨ÛŒØ´ Ø§Ø² Ø¯Ùˆ \n Ù…ØªÙˆØ§Ù„ÛŒ Ø¨Ù‡ Ø¯Ùˆ \n
        markdown_text = markdown_text.strip() # Ø­Ø°Ù ÙØ¶Ø§Ù‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ Ùˆ \n Ø§Ø² Ø§Ø¨ØªØ¯Ø§ Ùˆ Ø§Ù†ØªÙ‡Ø§ÛŒ Ú©Ù„ Ù…ØªÙ†
        
        # Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ø®Ø·ÙˆØ·ÛŒ Ú©Ù‡ ÙÙ‚Ø· Ø´Ø§Ù…Ù„ RLM Ù‡Ø³ØªÙ†Ø¯ Ø­Ø°Ù Ø´ÙˆÙ†Ø¯
        lines = markdown_text.splitlines()
        cleaned_lines = []
        for line in lines:
            stripped_line = line.strip()
            if stripped_line and stripped_line != self.RLM:
                cleaned_lines.append(stripped_line)
            elif not stripped_line and cleaned_lines and cleaned_lines[-1]: # Ø¨Ø±Ø§ÛŒ Ø­ÙØ¸ ÛŒÚ© Ø®Ø· Ø®Ø§Ù„ÛŒ Ø¨ÛŒÙ† Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§
                 cleaned_lines.append("")


        final_text = "\n".join(cleaned_lines)
        # Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù‡Ù†ÙˆØ² \n\n Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ÛŒ Ù…ØªÙ† Ø¨Ø§Ø´Ø¯ Ø§Ú¯Ø± Ø¢Ø®Ø±ÛŒÙ† Ø¨Ù„Ø§Ú© \n\n Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
        return final_text.strip()


    def format_event_message(self, event: EventInfo) -> str:
        description_md = self._prepare_description_for_markdown_v2(event.description)
        
        if not description_md.strip(): # Ø§Ú¯Ø± ØªÙˆØ¶ÛŒØ­Ø§Øª (Ú©Ù‡ Ø­Ø§Ù„Ø§ Ø´Ø§Ù…Ù„ Ø¹Ù†ÙˆØ§Ù† Ù‡Ù… Ù‡Ø³Øª) Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯
            logger.info(f"MarkdownV2 description is empty for event (Original title: {event.title[:30]}...). Skipping.")
            return ""

        # RLM Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ÛŒ Ú©Ù„ Ù¾ÛŒØ§Ù… Ø§Ú¯Ø± Ø¨Ø§ Ú©Ø§Ø±Ø§Ú©ØªØ± LTR Ø´Ø±ÙˆØ¹ Ø´ÙˆØ¯
        # Ø®ÙˆØ¯ description_md Ø¨Ø§ÛŒØ¯ Ø´Ø§Ù…Ù„ RLM Ù‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø¨Ø±Ø§ÛŒ Ø®Ø·ÙˆØ· Ø¯Ø§Ø®Ù„ÛŒ Ø¨Ø§Ø´Ø¯ (Ù…Ø«Ù„Ø§ Ø¨Ø±Ø§ÛŒ Ø¢ÛŒØªÙ… Ù‡Ø§ÛŒ Ù„ÛŒØ³Øª)
        message_prefix = self.RLM if description_md and not re.match(r"^\s*[\u0600-\u06FF*_[~`#]", description_md) else ""
        message_parts = [f"{message_prefix}{description_md}"]

        meta_info_parts = []
        if event.link:
            escaped_link_text = self._escape_md_v2("Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ø§Ù…Ù„ Ø±ÙˆÛŒØ¯Ø§Ø¯")
            safe_url = event.link.replace('(', '%28').replace(')', '%29')
            meta_info_parts.append(f"{self.RLM}ğŸ”— [{escaped_link_text}]({safe_url})")
        
        source_text_escaped = self._escape_md_v2(event.source_channel)
        if event.source_channel_username:
            tg_url = f"[https://t.me/](https://t.me/){event.source_channel_username}"
            meta_info_parts.append(f"{self.RLM}ğŸ“¢ *{self._escape_md_v2('Ù…Ù†Ø¨Ø¹:')}* [{source_text_escaped}]({tg_url})")
        else:
            meta_info_parts.append(f"{self.RLM}ğŸ“¢ *{self._escape_md_v2('Ù…Ù†Ø¨Ø¹:')}* {source_text_escaped}")

        if event.published:
            formatted_date_unescaped = event.published
            try:
                date_obj = datetime.strptime(event.published, "%a, %d %b %Y %H:%M:%S %Z")
                formatted_date_unescaped = date_obj.strftime("%d %b %Y - %H:%M (%Z)")
            except: pass
            if formatted_date_unescaped:
                 meta_info_parts.append(f"{self.RLM}ğŸ“… *{self._escape_md_v2('Ø§Ù†ØªØ´Ø§Ø±:')}* {self._escape_md_v2(formatted_date_unescaped)}")

        if meta_info_parts:
            message_parts.append("\n\n" + "\n".join(meta_info_parts)) # Ù‡Ù…ÛŒØ´Ù‡ Ø¯Ùˆ Ø®Ø· Ø¬Ø¯ÛŒØ¯ Ù‚Ø¨Ù„ Ø§Ø² Ø¨Ø®Ø´ Ù…ØªØ§

        final_message_md = "\n".join(message_parts).strip()
                                                          
        if len(final_message_md) > 4096:
             logger.warning(f"MarkdownV2 Message (Original title: '{event.title[:30]}') too long ({len(final_message_md)} chars).")
        return final_message_md

    async def publish_event(self, event: EventInfo):
        # (Ø§ÛŒÙ† Ù…ØªØ¯ Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø§Ø² Ù¾Ø§Ø³Ø® Ù‚Ø¨Ù„ÛŒ)
        try:
            message_md = self.format_event_message(event)
            if not message_md:
                logger.info(f"Skipping due to formatted message being empty for event from {event.source_channel} (Original Title: {event.title[:30]}...).")
                return

            await self.bot.send_message(
                chat_id=self.target_channel, text=message_md,
                parse_mode=ParseMode.MARKDOWN_V2,
                disable_web_page_preview=True 
            )
            logger.info(f"Published event (MarkdownV2): {event.title[:60]}... from {event.source_channel}")
        except Exception as e:
            logger.error(f"Failed to publish event ({event.title[:60]}...) using MarkdownV2 mode: {e}", exc_info=True)

    async def run_monitoring_loop(self):
        # (Ø§ÛŒÙ† Ù…ØªØ¯ Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø§Ø² Ù¾Ø§Ø³Ø® Ù‚Ø¨Ù„ÛŒØŒ Ø¨Ø§ check_interval_seconds=180)
        logger.info("Starting RSS monitoring...")
        await asyncio.sleep(10) 
        while True:
            logger.info("Checking all feeds...")
            all_new_events = []
            async with aiohttp.ClientSession() as session:
                tasks = [self.fetch_feed(session, feed_info) for feed_info in self.rss_feeds]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                for result in results:
                    if isinstance(result, list): all_new_events.extend(result)
                    elif isinstance(result, Exception): logger.error(f"Feed task error: {result}", exc_info=result)
            
            if all_new_events:
                logger.info(f"Found {len(all_new_events)} new event(s).")
                for event_to_publish in all_new_events:
                    await self.publish_event(event_to_publish)
                    await asyncio.sleep(5) 
            else:
                logger.info("No new events found.")
            
            check_interval_seconds = 180 
            logger.info(f"Next check in {check_interval_seconds // 60} minutes.")
            await asyncio.sleep(check_interval_seconds)


# ... (Ú©Ù„Ø§Ø³ Config Ùˆ ØªÙˆØ§Ø¨Ø¹ health_check, start_web_server, main Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø§Ø² Ù¾Ø§Ø³Ø® Ù‚Ø¨Ù„ÛŒ) ...
class Config:
    BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')
    TARGET_CHANNEL = os.getenv('TARGET_CHANNEL')

rss_bot_instance: Optional[RSSTelegramBot] = None 

async def health_check(request):
    global rss_bot_instance
    if rss_bot_instance:
        return web.Response(text=f"Bot is running! Processed items: {len(rss_bot_instance.processed_items)}", status=200)
    return web.Response(text="Bot not fully initialized.", status=200)

async def start_web_server():
    app = web.Application()
    app.router.add_get('/health', health_check)
    app.router.add_get('/', health_check)
    runner = web.AppRunner(app)
    await runner.setup()
    port = int(os.environ.get("PORT", "7860"))
    site = web.TCPSite(runner, '0.0.0.0', port)
    try:
        await site.start()
        logger.info(f"Web server started on port {port}")
    except OSError as e:
        logger.error(f"Failed to start web server on port {port}: {e}.")

async def main():
    global rss_bot_instance
    if not Config.BOT_TOKEN or not Config.TARGET_CHANNEL:
        logger.critical("Missing TELEGRAM_BOT_TOKEN or TARGET_CHANNEL environment variables!")
        return
    
    logger.info("Application starting...")
    rss_bot_instance = RSSTelegramBot(bot_token=Config.BOT_TOKEN, target_channel=Config.TARGET_CHANNEL)
    try:
        bot_info = await rss_bot_instance.bot.get_me()
        logger.info(f"Bot started: @{bot_info.username}")
        web_server_task = asyncio.create_task(start_web_server())
        await rss_bot_instance.run_monitoring_loop()
    except KeyboardInterrupt:
        logger.info("Bot stopped by user.")
    except Exception as e:
        logger.error(f"Critical bot error in main: {e}", exc_info=True)
    finally:
        logger.info("Bot shutting down...")
        if 'web_server_task' in locals() and web_server_task and not web_server_task.done(): 
            web_server_task.cancel()
            try: await web_server_task
            except asyncio.CancelledError: logger.info("Web server task cancelled.")
        logger.info("Application fully stopped.")

if __name__ == "__main__":
    asyncio.run(main())
